{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 4, in test", "AttributeError: '_io.TextIOWrapper' object has no attribute 'strip'", ""], "code": ["def test():", "    with open(\"./bug.csv\") as bug_file:", "        for line in bug_file:", "            line = bug_file.strip()"], "context": ["", "It has to be `line.strip()` not `compfile.strip()`", "```", "compfile = open(input('path:'), 'r')", "count = 0", "for line in compfile:", "    count += 1", "    print(\"Line{}: {}\".format(count, line.strip()))", "compfile.close()", "```", "", "The error is because you tried using `strip()` on the file object and it doesn't have the attribute `strip`", ""], "ideal": ["def test():", "    with open(\"./bug.csv\") as bug_file:", "        for line in bug_file:", "            line = line.strip()"], "context_url": "https://stackoverflow.com/questions/72669580/reading-line-by-line-is-showing-attributeerror", "context_author_url": ["https://stackoverflow.com/users/10804821/rohit-babu"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 26, in <module>", "    test()", "  File \"<string>\", line 7, in test", "AttributeError: 'numpy.ndarray' object has no attribute 'append'", ""], "code": ["import numpy as np", "", "def test():", "    x = np.arange(100)", "    train  = []", "    for i in range(10):", "        train.append(x[i:i+10])", "        train = np.vstack(train)", "    return train"], "context": ["", "```", "for col in range(loops):", "    train.append(D[:,col:col+x_len+y_len])", "    train = np.vstack(train)", "    X_train, Y_train = np.split(train, [x_len], axis = 1)", "```", "", "the error is in the code above.", "After `train = np.vstack(train)`, `train` becomes a numpy array, not a list, so in the next iteration you are going to append a value on a numpy array which doesn't have an `append` method, hence the error.", ""], "ideal": ["import numpy as np", "", "def test():", "    x = np.arange(100)", "    train  = []", "    for i in range(10):", "        train.append(x[i:i+10])", "    train = np.vstack(train)", "    return train"], "context_url": "https://stackoverflow.com/questions/68618417/attributeerror-numpy-ndarray-object-has-no-attribute-append-something-esc", "context_author_url": ["https://stackoverflow.com/users/4685471/desertnaut", "https://stackoverflow.com/users/15080349/husam-alsayed"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 5, in test", "TypeError: tile(): argument 'input' (position 1) must be Tensor, not torch.return_types.max", ""], "code": ["import torch", "", "def test():", "    x = torch.randn(4, 4)", "    print(torch.tile(torch.max(x, dim=1), (2, 2)))"], "context": ["", "For this error to be true, you have to be using some `dim=?`, because only then torch.max will return a tuple of `(values, indices)`.", "You can fix that error by using only the first output:", "```", "torch.tile(torch.max(x, dim=0)[0], (1, 1, 1, 5))", "```", "", ""], "ideal": ["import torch", "", "def test():", "    x = torch.randn(4, 4)", "    print(torch.tile(torch.max(x, dim=1)[0], (2, 2)))"], "context_url": "https://stackoverflow.com/questions/69109069/torch-return-types-max-as-tensor", "context_author_url": ["https://stackoverflow.com/users/4228275/berriel"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 28, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/pipeline.py\", line 446, in fit_transform", "    return last_step.fit_transform(Xt, y, **fit_params_last_step)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped", "    data_to_wrap = f(self, X, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped", "    data_to_wrap = f(self, X, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/base.py\", line 848, in fit_transform", "    return self.fit(X, **fit_params).transform(X)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped", "    data_to_wrap = f(self, X, *args, **kwargs)", "  File \"<string>\", line 20, in transform", "TypeError: 'numpy.ndarray' object is not callable", ""], "code": ["from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.base import BaseEstimator", "import pandas as pd", "from sklearn.pipeline import Pipeline", "from sklearn import datasets", "", "simple_cols = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']", "", "class ColumnSelectTransformer(BaseEstimator, TransformerMixin):", "    def __init__(self, columns):", "        self.columns = columns", "", "    def fit(self, X, y=None):", "        return self", "", "    def transform(self, X):", "        if not isinstance(X, pd.DataFrame):", "            X = pd.DataFrame(X)", "        X.dropna(inplace=True)", "        return X[self.columns].values()", "", "def test():", "    iris = datasets.load_iris(as_frame=True)", "    simple_features = Pipeline([", "        ('cst', ColumnSelectTransformer(simple_cols)),", "    ])", "    X = iris.data", "    simple_features.fit_transform(X)"], "context": ["", "As the log points out, the error is in `X[self.columns].values()`. `values` is a numpy array, so you cannot call it as a method (put parenthesis after it). You should try `X[self.columns].values`.", ""], "ideal": ["from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.base import BaseEstimator", "import pandas as pd", "from sklearn.pipeline import Pipeline", "from sklearn import datasets", "", "simple_cols = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']", "", "class ColumnSelectTransformer(BaseEstimator, TransformerMixin):", "    def __init__(self, columns):", "        self.columns = columns", "", "    def fit(self, X, y=None):", "        return self", "", "    def transform(self, X):", "        if not isinstance(X, pd.DataFrame):", "            X = pd.DataFrame(X)", "        X.dropna(inplace=True)", "        return X[self.columns].values", "", "def test():", "    iris = datasets.load_iris(as_frame=True)", "    simple_features = Pipeline([", "        ('cst', ColumnSelectTransformer(simple_cols)),", "    ])", "    X = iris.data", "    simple_features.fit_transform(X)"], "context_url": "https://stackoverflow.com/questions/58298416/scikit-learn-customtransformer-typeerror-numpy-ndarray-object-is-not-callabl", "context_author_url": ["https://stackoverflow.com/users/5687196/matteo-felici"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 8, in test", "RuntimeError: The expanded size of the tensor (-1) isn't allowed in a leading, non-existing dimension 1", ""], "code": ["import torch", "from torch import nn", "", "def test():", "    # create the tensor with all singleton dimensions in place", "    one_hot = torch.LongTensor([1, 0, 0, 0]) ", "    # create batch", "    one_hot.expand(32, -1, -1)", "    return one_hot"], "context": ["", "expand works along singleton dimensions of the input tensor. In your example, you are trying to `expand` a 1-by-18 tensor along its (non-existent) third and fourth dimensions - this is why you are getting an error. The only singleton dimension (=dimension with `size`==1) you have is the first dimension.", "fix ", "```", "one_hot = torch.zeros(1,18,1,1, dtype=torch.float)  # create the tensor with all singleton dimensions in place", "one_hot[0,1,0,0] = 1.", "one_hot.expand(-1,-1,40,40)", "```", "", ""], "ideal": ["import torch", "from torch import nn", "", "def test():", "    # create the tensor with all singleton dimensions in place", "    one_hot = torch.zeros(1,4,1, dtype=torch.float) ", "    one_hot[0, 1, 0] = 1", "    # create batch", "    one_hot.expand(32, -1, -1)", "    return one_hot"], "context_url": "https://stackoverflow.com/questions/53975352/using-expand-dims-in-pytorch", "context_author_url": ["https://stackoverflow.com/users/1714410/shai"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 5, in test", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/frame.py\", line 1764, in from_dict", "    return cls(data, index=index, columns=columns, dtype=dtype)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/frame.py\", line 664, in __init__", "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/internals/construction.py\", line 493, in dict_to_mgr", "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/internals/construction.py\", line 118, in arrays_to_mgr", "    index = _extract_index(arrays)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/internals/construction.py\", line 656, in _extract_index", "    raise ValueError(\"If using all scalar values, you must pass an index\")", "ValueError: If using all scalar values, you must pass an index", ""], "code": ["import pandas as pd", "", "def test():", "    data = {\"a\": 1, \"b\": 4}", "    df = pd.DataFrame.from_dict(data)"], "context": ["", "The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:", "```", ">>> df = pd.DataFrame({'A': [a], 'B': [b]})", ">>> df", "   A  B", "0  2  3", "```", "", "or use scalar values and pass an index:", "```", ">>> df = pd.DataFrame({'A': a, 'B': b}, index=[0])", ">>> df", "   A  B", "0  2  3", "```", "", ""], "ideal": ["import pandas as pd", "", "def test():", "    data = {\"a\": [1], \"b\": [4]}", "    df = pd.DataFrame.from_dict(data)"], "context_url": "https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi", "context_author_url": ["https://stackoverflow.com/users/487339/dsm"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 10, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_split.py\", line 2583, in train_test_split", "    train, test = next(cv.split(X=arrays[0], y=stratify))", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_split.py\", line 1688, in split", "    X, y, groups = indexable(X, y, groups)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 443, in indexable", "    check_consistent_length(*result)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 397, in check_consistent_length", "    raise ValueError(", "ValueError: Found input variables with inconsistent numbers of samples: [120, 150]", ""], "code": ["from sklearn import datasets", "from sklearn.model_selection import train_test_split ", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)", "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y)"], "context": ["", "The problem is in the `stratify` argument. You are using `stratify=y`, where you must use `stratify=y_train`. If not, you reach the inconsistent number of samples error. Try the code below:", "```", "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)", "```", "", ""], "ideal": ["from sklearn import datasets", "from sklearn.model_selection import train_test_split ", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)", "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"], "context_url": "https://stackoverflow.com/questions/66473716/sklearn-train-test-split-reporting-error-when-running-twice", "context_author_url": ["https://stackoverflow.com/users/4685471/desertnaut", "https://stackoverflow.com/users/14870925/alex-serra-marrugat"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 9, in test", "OSError: [Errno 39] Directory not empty: '/tmp/my'", ""], "code": ["import os", "from pathlib import Path", "", "def test():", "    p = Path(\"/tmp/my/directory\").mkdir(parents=True, exist_ok=True)", "    with open(\"/tmp/my/directory/cat.txt\", 'w') as f:", "        f.write(\"my password\")", "", "    os.rmdir(\"/tmp/my\")"], "context": ["", "Use shutil.rmtree:", "```", "import shutil", "", "shutil.rmtree(path)", "```", "", "See the documentation for details of how to handle and/or ignore errors.", ""], "ideal": ["import os", "from pathlib import Path", "import shutil", "", "def test():", "    p = Path(\"/tmp/my/directory\").mkdir(parents=True, exist_ok=True)", "    with open(\"/tmp/my/directory/cat.txt\", 'w') as f:", "        f.write(\"my password\")", "", "    shutil.rmtree(\"/tmp/my\")"], "context_url": "https://stackoverflow.com/questions/1557351/python-delete-non-empty-dir", "context_author_url": ["https://stackoverflow.com/users/826983/stefan-falk", "https://stackoverflow.com/users/21886/richiehindle"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 8, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize", "    sentences = [text] if preserve_line else sent_tokenize(text, language)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/__init__.py\", line 107, in sent_tokenize", "    return tokenizer.tokenize(text)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1276, in tokenize", "    return list(self.sentences_from_text(text, realign_boundaries))", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1332, in sentences_from_text", "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1332, in <listcomp>", "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1322, in span_tokenize", "    for sentence in slices:", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1421, in _realign_boundaries", "    for sentence1, sentence2 in _pair_iter(slices):", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 318, in _pair_iter", "    prev = next(iterator)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1395, in _slices_from_text", "    for match, context in self._match_potential_end_contexts(text):", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/nltk/tokenize/punkt.py\", line 1375, in _match_potential_end_contexts", "    for match in reversed(list(self._lang_vars.period_context_re().finditer(text))):", "TypeError: cannot use a string pattern on a bytes-like object", ""], "code": ["import nltk", "nltk.download('punkt')", "from urllib.request import urlopen", "", "def test():", "    url = \"http://www.google.com\"", "    raw = urlopen(url).read()", "    tokens = nltk.word_tokenize(raw)"], "context": ["", "You have to convert html (which is obtained as byte object) into a string using `decode('utf-8')`:", "```", ">>> import nltk", ">>> from urllib.request import urlopen", ">>> url = \"http://www.gutenberg.org/files/2554/2554.txt\"", ">>> raw = urlopen(url).read()", ">>> raw = raw.decode('utf-8')", ">>> tokens = nltk.word_tokenize(raw)", "```", "", ""], "ideal": ["import nltk", "nltk.download('punkt')", "from urllib.request import urlopen", "", "def test():", "    url = \"http://www.google.com\"", "    raw = urlopen(url).read().decode('utf-8')", "    tokens = nltk.word_tokenize(raw)"], "context_url": "https://stackoverflow.com/questions/42690716/error-using-nltk-word-tokenize", "context_author_url": ["https://stackoverflow.com/users/3824945/dmitry"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 4, in test", "NameError: name 'nn' is not defined", ""], "code": ["import torch", "", "def test():", "    model = nn.Sequential(", "        nn.Linear(10, 12),", "        nn.ReLU(),", "        nn.Linear(12, 3)", "    )", "", "    x = torch.randn(3, 10)", "    preds = model(x).detach().numpy()", "    return preds"], "context": ["", "If you got this error you can fix it with the following code:", "```", "import torch", "import torch.nn as nn", "```", "", "You need to include both lines, since if you set just the second one it may not work if the `torch` package is not imported.", "Where torch and torch.nn (or just `nn`) are two of the main PyTorch packages. You can `help(torch.nn)` to confirm this.", "It is not uncommon when you include `nn` to include the functional interface as `F` like this:", "```", "import torch", "import torch.nn as nn", "import torch.nn.functional as F", "```", "", "To bring you the hints what you imported or what is inside the `nn` package I provided the list:", "```", "['AdaptiveAvgPool1d', 'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d', 'AdaptiveLogSoftmaxWithLoss', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AlphaDropout', 'AvgPool1d', 'AvgPool2d', 'AvgPool3d', 'BCELoss', 'BCEWithLogitsLoss', 'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d', 'Bilinear', 'CELU', 'CTCLoss', 'ConstantPad1d', 'ConstantPad2d', 'ConstantPad3d', 'Container', 'Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d', 'CosineEmbeddingLoss', 'CosineSimilarity', 'CrossEntropyLoss', 'CrossMapLRN2d', 'DataParallel', 'Dropout', 'Dropout2d', 'Dropout3d', 'ELU', 'Embedding', 'EmbeddingBag', 'FeatureAlphaDropout', 'Fold', 'FractionalMaxPool2d', 'GLU', 'GRU', 'GRUCell', 'GroupNorm', 'Hardshrink', 'Hardtanh', 'HingeEmbeddingLoss', 'InstanceNorm1d', 'InstanceNorm2d', 'InstanceNorm3d', 'KLDivLoss', 'L1Loss', 'LPPool1d', 'LPPool2d', 'LSTM', 'LSTMCell', 'LayerNorm', 'LeakyReLU', 'Linear', 'LocalResponseNorm', 'LogSigmoid', 'LogSoftmax', 'MSELoss', 'MarginRankingLoss', 'MaxPool1d', 'MaxPool2d', 'MaxPool3d', 'MaxUnpool1d', 'MaxUnpool2d', 'MaxUnpool3d', 'Module', 'ModuleDict', 'ModuleList', 'MultiLabelMarginLoss', 'MultiLabelSoftMarginLoss', 'MultiMarginLoss', 'NLLLoss', 'NLLLoss2d', 'PReLU', 'PairwiseDistance', 'Parameter', 'ParameterDict', 'ParameterList', 'PixelShuffle', 'PoissonNLLLoss', 'RNN', 'RNNBase', 'RNNCell', 'RNNCellBase', 'RReLU', 'ReLU', 'ReLU6', 'ReflectionPad1d', 'ReflectionPad2d', 'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'SELU', 'Sequential', 'Sigmoid', 'SmoothL1Loss', 'SoftMarginLoss', 'Softmax', 'Softmax2d', 'Softmin', 'Softplus', 'Softshrink', 'Softsign', 'Tanh', 'Tanhshrink', 'Threshold', 'TripletMarginLoss', 'Unfold', 'Upsample', 'UpsamplingBilinear2d', 'UpsamplingNearest2d', 'ZeroPad2d', '_VF', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_functions', '_reduction', 'backends', 'functional', 'grad', 'init', 'modules', 'parallel', 'parameter', 'utils']", "```", "", "Containing many classes where probable the most fundamental one is the PyTorch class `nn.Module`.", "Do not confuse PyTorch class nn.Module with the Python modules.", "", "To fix the `SLL` model from the question you just have to add the first two lines: ", "```", "import torch", "import torch.nn as nn", "", "class SLL(nn.Module):", "    \"single linear layer\"", "    def __init__(self):", "        super().__init__()", "        self.l1 = nn.Linear(10,100)        ", "", "    def forward(self)->None: ", "        print(\"SLL:forward\")", "", "# we create a module instance m1", "m1 = SLL()", "```", "", "And you will get the output:", "```", "SLL(", "  (l1): Linear(in_features=10, out_features=100, bias=True)", ")", "```", "", ""], "ideal": ["import torch", "from torch import nn", "", "def test():", "    model = nn.Sequential(", "        nn.Linear(10, 12),", "        nn.ReLU(),", "        nn.Linear(12, 3)", "    )", "", "    x = torch.randn(3, 10)", "    preds = model(x).detach().numpy()", "    return preds"], "context_url": "https://stackoverflow.com/questions/56633138/nameerror-name-nn-is-not-defined", "context_author_url": ["https://stackoverflow.com/users/5884955/prosti"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 9, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/frame.py\", line 3968, in __setitem__", "    self._set_item_frame_value(key, value)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/frame.py\", line 4123, in _set_item_frame_value", "    raise ValueError(", "ValueError: Cannot set a DataFrame with multiple columns to the single column b/c", ""], "code": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {'a': [\"hello\", \"what\", \"how\", \"when\"], 'b': [4, 5, 6, 5], 'c': [7, 8, 9, -1], 'd': [1, 0, 0, -1]}", "    df = pd.DataFrame.from_dict(data)", "    df.rename(columns={\"d\": \"b\"}, inplace=True)", "", "    df['b/c'] = df['b'].div(df['c'].values, axis=0)", "    return df"], "context": ["", "Problem is duplicated columns names, verify:", "```", "#generate duplicates", "df = pd.concat([df, df], axis=1)", "print (df)", "  Cost Reve Cost Reve", "0    0    3    0    3", "1    4    0    4    0", "2    0    0    0    0", "3   10   10   10   10", "4    4    8    4    8", "", "df[['Cost','Reve']] = df[['Cost','Reve']].apply(pd.to_numeric)", "print (df)", "# ValueError: Columns must be same length as key", "```", "", "You can find this columns names:", "```", "print (df.columns[df.columns.duplicated(keep=False)])", "Index(['Cost', 'Reve', 'Cost', 'Reve'], dtype='object')", "```", "", "If same values in columns is possible remove duplicated by:", "```", "df = df.loc[:, ~df.columns.duplicated()]", "df[['Cost','Reve']] = df[['Cost','Reve']].apply(pd.to_numeric)", "", "#simplify division", "df['C/R'] = df['Cost'].div(df['Reve'])", "print (df)", "   Cost  Reve  C/R", "0     0     3  0.0", "1     4     0  inf", "2     0     0  NaN", "3    10    10  1.0", "4     4     8  0.5", "```", "", ""], "ideal": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {'a': [\"hello\", \"what\", \"how\", \"when\"], 'b': [4, 5, 6, 5], 'c': [7, 8, 9, -1], 'd': [1, 0, 0, -1]}", "    df = pd.DataFrame.from_dict(data)", "    df.rename(columns={\"d\": \"newb\"}, inplace=True)", "", "    df['newb/c'] = df['newb'].div(df['c'].values, axis=0)", "    return df"], "context_url": "https://stackoverflow.com/questions/61650474/valueerror-columns-must-be-same-length-as-key-in-pandas", "context_author_url": ["https://stackoverflow.com/users/2901002/jezrael"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 7, in test", "  File \"<string>\", line 2, in percentage", "ZeroDivisionError: division by zero", ""], "code": ["def percentage(a):", "    return sum(a[:-3]) / sum(a) ", "", "def test():", "    a = [1, 2, 3, -1, -3, -2]", "", "    return percentage(a)"], "context": ["", "The problem is obvious if you print out the values of `part` and `whole` that caused this error to occur.", "The solution is to handle any Division by Zero errors like so", "```", "       try:", "           result = percentage(sum(list), total)", "       except ZeroDivisionError:", "           # Handle the error in whatever way makes sense for your application", "```", "", "Alternatively, you can check for zero before you divide", "```", "def percentage(part,whole):", "    if whole == 0:", "        if part == 0:", "            return float(\"nan\")", "        return float(\"inf\")", "    return float(part) / float(whole) *100", "```", "", "(Thank you Joran Beasley and Max for making this mathematically correct)", ""], "ideal": ["def percentage(a):", "    try:", "        result = sum(a[:-3]) / sum(a) ", "    except ZeroDivisionError:", "        result = float(\"inf\")", "    return result", "", "def test():", "    a = [1, 2, 3, -1, -3, -2]", "", "    return percentage(a)"], "context_url": "https://stackoverflow.com/questions/53123918/strange-error-zerodivisionerror-float-division-by-zero", "context_author_url": ["https://stackoverflow.com/users/786593/hostingutilities-com"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 4, in test", "RuntimeError: dictionary changed size during iteration", ""], "code": ["def test():", "    word_cnt = {\"apple\": 324, \"a\": 2045, \"doctor\": 23, \"dasda\": 1, \"dasa\": 2}", "", "    for i in word_cnt.keys():", "        if word_cnt[i] < 5:", "            word_cnt.pop(i)", "    return word_cnt"], "context": ["", "In Python 3.x and 2.x you can use use list to force a copy of the keys to be made:", "```", "for i in list(d):", "```", "", "In Python 2.x calling .keys made a copy of the keys that you could iterate over while modifying the `dict`:", "```", "for i in d.keys():", "```", "", "but on Python 3.x, `.keys` returns a view object instead, so it won't fix your error.", ""], "ideal": ["def test():", "    word_cnt = {\"apple\": 324, \"a\": 2045, \"doctor\": 23, \"dasda\": 1, \"dasa\": 2}", "", "    for i in list(word_cnt.keys()):", "        if word_cnt[i] < 5:", "            word_cnt.pop(i)", "    return word_cnt"], "context_url": "https://stackoverflow.com/questions/11941817/how-can-i-avoid-runtimeerror-dictionary-changed-size-during-iteration-error", "context_author_url": ["https://stackoverflow.com/users/61974/mark-byers"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 27, in <module>", "    exec(\"\\n\".join(data[\"code\"]))", "  File \"<string>\", line 1, in <module>", "ImportError: cannot import name 'cross_validation' from 'sklearn' (/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/__init__.py)", ""], "code": ["from sklearn import cross_validation", "from sklearn import datasets", "", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(", "        X, y, test_size=0.2, stratify=y", "    )"], "context": ["", "You are getting this error because `train_test_split` is now in `model_selection` module of `sklearn`. You can see the change log over here.", "You can import it like this now. ", "```", "from sklearn.model_selection import train_test_split", "```", "", "and use it like this", "```", "X_train, X_test, y_train, y_test = train_test_split(X, y, ", "test_size=0.2)", "```", "", ""], "ideal": ["from sklearn.model_selection import train_test_split", "from sklearn import datasets", "", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "    X_train, X_test, y_train, y_test = train_test_split(", "        X, y, test_size=0.2, stratify=y", "    )"], "context_url": "https://stackoverflow.com/questions/54005445/im-stuck-on-with-an-attribute-error-with-sklearn", "context_author_url": ["https://stackoverflow.com/users/7896849/sreeram-tp"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 20, in <module>", "    exec(\"\\n\".join(data[\"code\"]))", "  File \"<string>\", line 20, in <module>", "AttributeError: Can't get attribute 'MyCLS' on <module '__main__' from 'error.py'>", ""], "code": ["import pickle", "", "", "class MyCLS:", "    code = \"100\"", "    name = \"my_cls\"", "", "    def __init__(self, subname):", "        self.subname = subname", "", "my1 = MyCLS(1)", "my2 = MyCLS(2)", "", "with open(\"test\", \"wb\") as outfile:", "    pickle.dump([my1, my2], outfile)", "", "del MyCLS, my1, my2", "", "with open(\"test\", \"rb\") as f:", "    W = pickle.load(f)"], "context": ["", "I believe the error is caused because you deleted the class definition. Object serialization in Python (which to my knowledge is also in Java) requires the class definition to be there.", "From your linked documentation:", "> ", "> Note that functions (built-in and user-defined) are pickled by “fully qualified” name reference, not by value. This means that only the function name is pickled, along with the name of the module the function is defined in. Neither the function’s code, nor any of its function attributes are pickled. Thus the defining module must be importable in the unpickling environment, and the module must contain the named object, otherwise an exception will be raised. [4]> ", "> Similarly, classes are pickled by named reference, so the same restrictions in the unpickling environment apply. Note that none of the class’s code or data is pickled", "> ", "", "If you want to send your friend the class and instances, send the class through a code defining the class `wow`, and the instances through the pickle file.", ""], "ideal": ["import pickle", "", "", "class MyCLS:", "    code = \"100\"", "    name = \"my_cls\"", "", "    def __init__(self, subname):", "        self.subname = subname", "", "my1 = MyCLS(1)", "my2 = MyCLS(2)", "", "with open(\"test\", \"wb\") as outfile:", "    pickle.dump([my1, my2], outfile)", "", "del my1, my2", "", "with open(\"test\", \"rb\") as f:", "    W = pickle.load(f)"], "context_url": "https://stackoverflow.com/questions/34261379/how-to-recover-a-pickled-class-and-its-instances", "context_author_url": ["https://stackoverflow.com/users/-1/community", "https://stackoverflow.com/users/895932/justhalf"]}
{"error": ["Process Process-4:", "Traceback (most recent call last):", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/managers.py\", line 827, in _callmethod", "    conn = self._tls.connection", "AttributeError: 'ForkAwareLocal' object has no attribute 'connection'", "", "During handling of the above exception, another exception occurred:", "", "Traceback (most recent call last):", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap", "    self.run()", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/process.py\", line 108, in run", "    self._target(*self._args, **self._kwargs)", "  File \"<string>\", line 7, in launch_worker", "  File \"<string>\", line 2, in __setitem__", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/managers.py\", line 831, in _callmethod", "    self._connect()", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/managers.py\", line 818, in _connect", "    conn = self._Client(self._token.address, authkey=self._authkey)", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/connection.py\", line 509, in Client", "    deliver_challenge(c, authkey)", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/connection.py\", line 740, in deliver_challenge", "    response = connection.recv_bytes(256)        # reject large message", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes", "    buf = self._recv_bytes(maxlength)", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes", "    buf = self._recv(4)", "  File \"/anaconda/envs/torch13/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv", "    chunk = read(handle, remaining)", "ConnectionResetError: [Errno 104] Connection reset by peer"], "code": ["from multiprocessing import Process, Manager, Lock", "", "lock = Lock()", "def launch_worker(d, index):", "    global lock", "    lock.acquire()", "    d[index] = \"new\"", "    print(d[index])", "    lock.release()", "    return None", "", "", "def test():", "    i = 1", "    d = {}", "    mp = Manager()", "    d = mp.dict()", "    d[1] = \"a\"", "    d[2] = \"b\"", "    d[3] = \"c\"", "    d[4] = \"d\"", "    d[5] = \"e\"", "", "    for i in range(4):", "        i = i + 1", "        p = Process(target=launch_worker, args=(d, i))", "        p.start()", "", "    return None"], "context": ["", "The dict your workers modify is a shared object managed by the dispatching process; modifications to that object by the workers requires that they communicate with the dispatching process. The errors you see come from the fact that your dispatcher isn't waiting for the worker processes after it launches them; it's exiting too soon, so it might not exist for them to communicate with when they need to. ", "The first worker or two that attempts to update the shared dict might succeed, because when they modify the shared dict the process containing the `Manager` instance might still exist (e.g., it might still be in the process of creating further workers). Thus in your examples you see some successful output.  But the managing process soon exits, and the next worker that attempts a modification will fail. ", "What you need to do is call the `join` method on the `Process` objects as a way of waiting for each of them to exit.  The following modification of your `dispatcher` shows the basic idea:", "```", "def dispatcher():", "    mp = Manager()", "    d = mp.dict()", "    d[1] = \"a\"", "    d[2] = \"b\"", "    d[3] = \"c\"", "    d[4] = \"d\"", "    d[5] = \"e\"", "    procs = []", "    for i in range(4):", "        p = Process(target=launch_worker, args=(d,i))", "        procs.append(p)", "        p.start()", "    for p in procs:", "        p.join()", "```", "", ""], "ideal": ["from multiprocessing import Process, Manager, Lock", "", "lock = Lock()", "def launch_worker(d, index):", "    global lock", "    lock.acquire()", "    d[index] = \"new\"", "    print(d[index])", "    lock.release()", "    return None", "", "", "def test():", "    i = 1", "    d = {}", "    mp = Manager()", "    d = mp.dict()", "    d[1] = \"a\"", "    d[2] = \"b\"", "    d[3] = \"c\"", "    d[4] = \"d\"", "    d[5] = \"e\"", "    procs = []", "", "    for i in range(4):", "        i = i + 1", "        p = Process(target=launch_worker, args=(d, i))", "        procs.append(p)", "        p.start()", "", "    for p in procs:", "        p.join()", "", "    return None"], "context_url": "https://stackoverflow.com/questions/21420413/python-multiprocessing-lock-mechanism-failing-when-acquired-lock", "context_author_url": ["https://stackoverflow.com/users/1515737/alp"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 23, in <module>", "    exec(\"\\n\".join(data[\"code\"]))", "  File \"<string>\", line 2", "    def add(lambda, gamma):", "            ^", "SyntaxError: invalid syntax", ""], "code": ["def test():", "    def add(lambda, gamma):", "        return lambda + gamma", "    ", "    return add(1, 1)"], "context": ["", "`lambda` is a reserved word in python, and it shouldn't be used as a parameter/variable name. This goes the same with all other reserved words.", ""], "ideal": ["def test():", "    def add(alpha, gamma):", "        return alpha + gamma", "    ", "    return add(1, 1)"], "context_url": "https://stackoverflow.com/questions/46110028/python-lambda-syntax-error", "context_author_url": ["https://stackoverflow.com/users/6838049/afagarap"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 8, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl", "    return forward_call(*input, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/sparse.py\", line 158, in forward", "    return F.embedding(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/functional.py\", line 2199, in embedding", "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)", "IndexError: index out of range in self", ""], "code": ["import torch", "from torch import nn", "", "def test():", "    word_dict = {\"hello\": 0, \"world\": 1, \"how\":2, \"are\":3, \"you\":4}", "    embeds = nn.Embedding(2, 100)", "    lookup_tensor = torch.tensor(word_dict[\"how\"], dtype=torch.long)", "    hello_embed = embeds(lookup_tensor)", "    print(hello_embed)"], "context": ["", "When you declare `embeds = nn.Embedding(2, 5)` the vocab size is 2 and embedding size is 5. i.e each word will be represented by a vector of size 5 and there are only 2 words in vocab.", "`lookup_tensor = torch.tensor(word_to_ix[\"how\"], dtype=torch.long)` embeds will try to look up vector corresponding to the third word in vocab, but embedding has vocab size of 2. and that is why you get the error.", "If you declare `embeds = nn.Embedding(5, 5)` it should work fine.", ""], "ideal": ["import torch", "from torch import nn", "", "def test():", "    word_dict = {\"hello\": 0, \"world\": 1, \"how\":2, \"are\":3, \"you\":4}", "    embeds = nn.Embedding(5, 100)", "    lookup_tensor = torch.tensor(word_dict[\"how\"], dtype=torch.long)", "    hello_embed = embeds(lookup_tensor)", "    print(hello_embed)"], "context_url": "https://stackoverflow.com/questions/51456059/pytorch-nn-embedding-error", "context_author_url": ["https://stackoverflow.com/users/3236925/umang-gupta"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 7, in test", "KeyError: 'easy'", ""], "code": ["def test():", "    rslt = {}", "    inputs = {\"easy\": [[1, 1], [3, 4]], \"hard\": [[1234, -232], [123.21, 21.31]]}", "    ", "    for k in inputs:", "        for i in inputs[k]:", "            rslt[k].append(sum(i))", "", "    return rslt"], "context": ["", "As I understand, `return_data` is another dict. If so, it doesn't (as of yet) have a key named `fpath` (which is 'sig9'). Hence, the error. ", "To avoid it, you should either use defaultdict, or initialize this element as an empty list every time you come across a new key.", ""], "ideal": ["from collections import defaultdict", "def test():", "    rslt = defaultdict(list)", "    inputs = {\"easy\": [[1, 1], [3, 4]], \"hard\": [[1234, -232], [123.21, 21.31]]}", "    ", "    for k in inputs:", "        for i in inputs[k]:", "            rslt[k].append(sum(i))", "", "    return rslt"], "context_url": "https://stackoverflow.com/questions/22926323/key-error-in-dictionary-python", "context_author_url": ["https://stackoverflow.com/users/-1/community", "https://stackoverflow.com/users/1564449/sashkello"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 26, in <module>", "    test()", "  File \"<string>\", line 7, in test", "AssertionError: np.delete doesn't work", ""], "code": ["import numpy as np", "", "def test():", "    x = np.arange(100)", "    np.delete(x, 10)", "", "    assert x[10] != 10, \"np.delete doesn't work\""], "context": ["", "`numpy.delete()` is not an in-place operation, and returns a new copy of your array that has been operated on. As a result, `ponto` itself is never changed, you just return a changed copy of it and do nothing with it. Assign a new variable to your `numpy.delete()` statements, and print that. ", "Also note `try-except-pass` is inherently evil and you could in fact have 100 things wrong in your code without knowing as it will hide all the errors. ", "You can find the documentation concerning `numpy.delete` which explains this behavior here.", ""], "ideal": ["import numpy as np", "", "def test():", "    x = np.arange(100)", "    x = np.delete(x, 10)", "", "    assert x[10] != 10, \"np.delete doesn't work\""], "context_url": "https://stackoverflow.com/questions/29286465/python-numpy-delete-doesnt-work", "context_author_url": ["https://stackoverflow.com/users/4686625/miradulo"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 27, in <module>", "    test()", "  File \"<string>\", line 13, in test", "TypeError: can't multiply sequence by non-int of type 'numpy.float64'", ""], "code": ["import matplotlib.pyplot as plt", "import numpy as np", "", "def test():", "    x = [0.46,0.59,0.68,0.99,0.39,0.31,1.09,0.77,0.72,0.49,0.55,0.62,0.58,0.88,0.78]", "    y = [0.315,0.383,0.452,0.650,0.279,0.215,0.727,0.512,0.478,0.335,0.365,0.424,0.390,0.585,0.511]", "    xerr = [0.01]*15", "    yerr = [0.001]*15", "", "    plt.rc('font', family='serif', size=13)", "    m, b = np.polyfit(x, y, 1)", "    plt.plot(x,y,'s',color='#0066FF')", "    plt.plot(x, m*x + b, 'r-') #BREAKS ON THIS LINE", "    plt.errorbar(x,y,xerr=xerr,yerr=0,linestyle=\"None\",color='black')", "    plt.xlabel('$\\Delta t$ $(s)$',fontsize=20)", "    plt.ylabel('$\\Delta p$ $(hPa)$',fontsize=20)", "    plt.autoscale(enable=True, axis=u'both', tight=False)", "    plt.grid(False)", "    plt.xlim(0.2,1.2)", "    plt.ylim(0,0.8)", "    plt.show()"], "context": ["", "You should make `x` and `y` numpy arrays, not lists:", "```", "x = np.array([0.46,0.59,0.68,0.99,0.39,0.31,1.09,", "              0.77,0.72,0.49,0.55,0.62,0.58,0.88,0.78])", "y = np.array([0.315,0.383,0.452,0.650,0.279,0.215,0.727,0.512,", "              0.478,0.335,0.365,0.424,0.390,0.585,0.511])", "```", "", "With this change, it produces the expected plot. If they are lists, `m * x` will not produce the result you expect, but an empty list. Note that `m` is  a`numpy.float64` scalar, not a standard Python `float`.", "I actually consider this a bit dubious behavior of Numpy. In normal Python, multiplying a list with an integer just repeats the list:", "```", "In [42]: 2 * [1, 2, 3]", "Out[42]: [1, 2, 3, 1, 2, 3]", "```", "", "while multiplying a list with a float gives an error (as I think it should):", "```", "In [43]: 1.5 * [1, 2, 3]", "---------------------------------------------------------------------------", "TypeError                                 Traceback (most recent call last)", "<ipython-input-43-d710bb467cdd> in <module>()", "----> 1 1.5 * [1, 2, 3]", "TypeError: can't multiply sequence by non-int of type 'float'", "```", "", "The weird thing is that multiplying a Python list with a Numpy scalar apparently works:", "```", "In [45]: np.float64(0.5) * [1, 2, 3]", "Out[45]: []", "", "In [46]: np.float64(1.5) * [1, 2, 3]", "Out[46]: [1, 2, 3]", "", "In [47]: np.float64(2.5) * [1, 2, 3]", "Out[47]: [1, 2, 3, 1, 2, 3]", "```", "", "So it seems that the float gets truncated to an int, after which you get the standard Python behavior of repeating the list, which is quite unexpected behavior. The best thing would have been to raise an error (so that you would have spotted the problem yourself instead of having to ask your question on Stackoverflow) or to just show the expected element-wise multiplication (in which your code would have just worked). Interestingly, addition between a list and a Numpy scalar does work:", "```", "In [69]: np.float64(0.123) + [1, 2, 3]", "Out[69]: array([ 1.123,  2.123,  3.123])", "```", "", ""], "ideal": ["import matplotlib.pyplot as plt", "import numpy as np", "", "def test():", "    x = np.array([0.46,0.59,0.68,0.99,0.39,0.31,1.09,0.77,0.72,0.49,0.55,0.62,0.58,0.88,0.78])", "    y = np.array([0.315,0.383,0.452,0.650,0.279,0.215,0.727,0.512,0.478,0.335,0.365,0.424,0.390,0.585,0.511])", "    xerr = [0.01]*15", "    yerr = [0.001]*15", "", "    plt.rc('font', family='serif', size=13)", "    m, b = np.polyfit(x, y, 1)", "    plt.plot(x,y,'s',color='#0066FF')", "    plt.plot(x, m*x + b, 'r-') #BREAKS ON THIS LINE", "    plt.errorbar(x,y,xerr=xerr,yerr=0,linestyle=\"None\",color='black')", "    plt.xlabel('$\\Delta t$ $(s)$',fontsize=20)", "    plt.ylabel('$\\Delta p$ $(hPa)$',fontsize=20)", "    plt.autoscale(enable=True, axis=u'both', tight=False)", "    plt.grid(False)", "    plt.xlim(0.2,1.2)", "    plt.ylim(0,0.8)", "    plt.show()"], "context_url": "https://stackoverflow.com/questions/26690480/matplotlib-typeerror-cant-multiply-sequence-by-non-int-of-type-numpy-float64", "context_author_url": ["https://stackoverflow.com/users/2647279/bas-swinckels"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 2, in test", "NameError: name 'reduce' is not defined", ""], "code": ["def test():", "    return reduce(lambda x,y: (x+y)%2, range(100))"], "context": ["", "It was moved to functools.", ""], "ideal": ["from functools import reduce", "def test():", "    return reduce(lambda x,y: (x+y)%2, range(100))"], "context_url": "https://stackoverflow.com/questions/8689184/nameerror-name-reduce-is-not-defined-in-python", "context_author_url": ["https://stackoverflow.com/users/20862/ignacio-vazquez-abrams"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 2, in test", "UnboundLocalError: local variable 'list' referenced before assignment", ""], "code": ["def test():", "    list0 = list('easyhoss') ", "    list = list(\"abs\")", "    list2 = list(\"abs3\")"], "context": ["", "Seems like you've shadowed the builtin name list, which points at a class, by the same name pointing at an instance of it. Here is an example:", "```", ">>> example = list('easyhoss')  # here `list` refers to the builtin class", ">>> list = list('abc')  # we create a variable `list` referencing an instance of `list`", ">>> example = list('easyhoss')  # here `list` refers to the instance", "Traceback (most recent call last):", "  File \"<string>\", line 1, in <module>", "TypeError: 'list' object is not callable", "```", "", "I believe this is fairly obvious. Python stores object names (functions and classes are objects, too) in namespaces (which are implemented as dictionaries), hence you can rewrite pretty much any name in any scope. It won't show up as an error of some sort. As you might know, Python emphasizes that \"special cases aren't special enough to break the rules\". And there are two major rules behind the problem you've faced:", "", "Namespaces. Python supports nested namespaces. Theoretically you can endlessly nest them. As I've already mentioned, they are basically dictionaries of names and references to corresponding objects. Any module you create gets its own \"global\" namespace, though in fact it's just a local namespace with respect to that particular module.", "", "Scoping. When you reference a name, the Python runtime looks it up in the local namespace (with respect to the reference) and, if such name does not exist, it repeats the attempt in a higher-level namespace. This process continues until there are no higher namespaces left. In that case you get a NameError. Builtin functions and classes reside in a special high-order namespace __builtins__. If you declare a variable named list in your module's global namespace, the interpreter will never search for that name in a higher-level namespace (that is __builtins__). Similarly, suppose you create a variable var inside a function in your module, and another variable var in the module. Then, if you reference var inside the function, you will never get the global var, because there is a var in the local namespace - the interpreter has no need to search it elsewhere.", "", "", "Here is a simple illustration.", "```", ">>> example = list(\"abc\")  # Works fine", ">>> ", ">>> # Creating name \"list\" in the global namespace of the module", ">>> list = list(\"abc\")", ">>> ", ">>> example = list(\"abc\")", "Traceback (most recent call last):", "  File \"<stdin>\", line 1, in <module>", "TypeError: 'list' object is not callable", ">>> # Python looks for \"list\" and finds it in the global namespace,", ">>> # but it's not the proper \"list\".", ">>> ", ">>> # Let's remove \"list\" from the global namespace", ">>> del list", ">>> # Since there is no \"list\" in the global namespace of the module,", ">>> # Python goes to a higher-level namespace to find the name. ", ">>> example = list(\"abc\")  # It works.", "```", "", "So, as you see there is nothing special about Python builtins. And your case is a mere example of universal rules. You'd better use an IDE (e.g. a free version of PyCharm, or Atom with Python plugins) that highlights name shadowing to avoid such errors.", "You might as well be wondering what is a \"callable\", in which case you can read this post. `list`, being a class, is callable. Calling a class triggers instance construction and initialisation. An instance might as well be callable, but `list` instances are not. If you are even more puzzled by the distinction between classes and instances, then you might want to read the documentation (quite conveniently, the same page covers namespaces and scoping).", "If you want to know more about builtins, please read the answer by Christian Dean.", "P.S. When you start an interactive Python session, you create a temporary module.", ""], "ideal": ["def test():", "    list0 = list('easyhoss') ", "    list1 = list(\"abs\")", "    list2 = list(\"abs3\")"], "context_url": "https://stackoverflow.com/questions/31087111/why-does-example-list-result-in-typeerror-list-object-is-not-callab", "context_author_url": ["https://stackoverflow.com/users/4518341/wjandrea", "https://stackoverflow.com/users/3846213/eli-korvigo"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 4, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/pathlib.py\", line 1288, in mkdir", "    self._accessor.mkdir(self, mode)", "FileExistsError: [Errno 17] File exists: '/tmp/my/directory'", ""], "code": ["from pathlib import Path", "", "def test():", "    Path(\"/tmp/my/directory\").mkdir()"], "context": ["", "On Python ≥ 3.5, use pathlib.Path.mkdir:", "```", "from pathlib import Path", "Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)", "```", "", "For older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:", "Try os.path.exists, and consider os.makedirs for the creation.", "```", "import os", "if not os.path.exists(directory):", "    os.makedirs(directory)", "```", "", "As noted in comments and elsewhere, there's a race condition – if the directory is created between the `os.path.exists` and the `os.makedirs` calls, the `os.makedirs` will fail with an `OSError`. Unfortunately, blanket-catching `OSError` and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.", "One option would be to trap the `OSError` and examine the embedded error code (see Is there a cross-platform way of getting information from Python’s OSError):", "```", "import os, errno", "", "try:", "    os.makedirs(directory)", "except OSError as e:", "    if e.errno != errno.EEXIST:", "        raise", "```", "", "Alternatively, there could be a second `os.path.exists`, but suppose another created the directory after the first check, then removed it before the second one – we could still be fooled. ", "Depending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.", "Modern versions of Python improve this code quite a bit, both by exposing FileExistsError (in 3.3+)...", "```", "try:", "    os.makedirs(\"path/to/directory\")", "except FileExistsError:", "    # directory already exists", "    pass", "```", "", "...and by allowing a keyword argument to os.makedirs called exist_ok (in 3.2+).", "```", "os.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.", "```", "", ""], "ideal": ["from pathlib import Path", "", "def test():", "    Path(\"/tmp/my/directory\").mkdir(parents=True, exist_ok=True)"], "context_url": "https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories", "context_author_url": ["https://stackoverflow.com/users/1199/blair-conrad"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 2, in test", "TypeError: unhashable type: 'set'", ""], "code": ["def test():", "    return set().union({n, 2*n} for n in range(5))"], "context": ["", "The reason for this error is that `set.union()` expects one or more sets (ie `set.union(oneset, anotherset, andathirdone)`), not a `list` nor generator.", "The solution is to unpack your list or generator:", "```", ">>> set().union( *({n, 2*n} for n in range(5)) )", "{0, 1, 2, 3, 4, 6, 8}", "```", "", ""], "ideal": ["def test():", "    return set().union(*({n, 2*n} for n in range(5)))"], "context_url": "https://stackoverflow.com/questions/51871836/python-union-of-sets-raises-typeerror", "context_author_url": ["https://stackoverflow.com/users/41316/bruno-desthuilliers", "https://stackoverflow.com/users/8708364/u13-forward"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 25, in test", "AttributeError: 'RandomForestRegressor' object has no attribute 'best_params_'", ""], "code": ["from sklearn.ensemble import RandomForestRegressor", "from sklearn.model_selection import GridSearchCV", "import numpy as np", "", "def test():", "    rf2 = RandomForestRegressor(random_state = 50)", "", "    num_leafs = [1, 5, 10, 20, 50, 100]", "    parameters3 = [{'n_estimators' : range(100,800,20),", "                'max_depth': range(1,20,2),", "                'min_samples_leaf':num_leafs", "                }]", "", "", "    gs3 = GridSearchCV(estimator=rf2,", "                    param_grid=parameters3,", "                    cv = 10,", "                    n_jobs = -1)", "", "    X_train = np.random.randn(32, 2)", "    y_train = np.random.randn(32)", "", "    gs3 = rf2.fit(X_train, y_train)", "", "    print(gs3.best_params_)"], "context": ["", "Replace this:", "`gs3 = rf2.fit(X_train1, y_train1)`", "by this:", "`gs3.fit(X_train1, y_train1)`", "Then you'll be able to use: ", "`gs3.best_params_`", "You error was caused by the fact that you reassigned `gs3` to the `RandomForest()` call, so `gs3` wasn't a `GridSearchCV` object anymore.", ""], "ideal": ["from sklearn.ensemble import RandomForestRegressor", "from sklearn.model_selection import GridSearchCV", "import numpy as np", "", "def test():", "    rf2 = RandomForestRegressor(random_state = 50)", "", "    num_leafs = [1, 5, 10, 20, 50, 100]", "    parameters3 = [{'n_estimators' : range(100,800,20),", "                'max_depth': range(1,20,2),", "                'min_samples_leaf':num_leafs", "                }]", "", "", "    gs3 = GridSearchCV(estimator=rf2,", "                    param_grid=parameters3,", "                    cv = 10,", "                    n_jobs = -1)", "", "    X_train = np.random.randn(32, 2)", "    y_train = np.random.randn(32)", "", "    gs3.fit(X_train, y_train)", "", "    print(gs3.best_params_)"], "context_url": "https://stackoverflow.com/questions/58900790/attribute-error-randomforestregressor-object-has-no-attribute-best-params", "context_author_url": ["https://stackoverflow.com/users/10908375/nicolas-gervais"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 8, in test", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/series.py\", line 206, in wrapper", "    raise TypeError(f\"cannot convert the series to {converter}\")", "TypeError: cannot convert the series to <class 'int'>", ""], "code": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {\"a\": ['1', '3', '4', '5', '6'], \"b\": ['is', 'am', 'island', 'icream', np.nan]}", "    df = pd.DataFrame.from_dict(data)", "    ", "    df[\"a\"] = int(df['a'])"], "context": ["", "Your error is on line 2.  `df['intage'] = int(df['age'])` is not valid, and you can't pass a pandas series to the int function.", "You need to use `astype` if df['age'] is object dtype.", "```", "df['intage'] = df['age'].astype(int)", "```", "", "Or since you are subtracting two dates, you need to use the dt accessor with the days attribute to get the number of days as an integer:", "```", "df['intage'] = df['age'].dt.days", "```", "", ""], "ideal": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {\"a\": ['1', '3', '4', '5', '6'], \"b\": ['is', 'am', 'island', 'icream', np.nan]}", "    df = pd.DataFrame.from_dict(data)", "    ", "    df[\"a\"] = df['a'].astype(int)"], "context_url": "https://stackoverflow.com/questions/51865367/cannot-convert-the-series-to-class-int", "context_author_url": ["https://stackoverflow.com/users/63550/peter-mortensen", "https://stackoverflow.com/users/6361531/scott-boston"]}
{"error": ["Traceback (most recent call last):", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/indexing.py\", line 873, in _validate_tuple_indexer", "    self._validate_key(k, i)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1483, in _validate_key", "    raise ValueError(f\"Can only index by location with a [{self._valid_types}]\")", "ValueError: Can only index by location with a [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array]", "", "The above exception was the direct cause of the following exception:", "", "Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 10, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1067, in __getitem__", "    return self._getitem_tuple(key)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1563, in _getitem_tuple", "    tup = self._validate_tuple_indexer(tup)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/indexing.py\", line 875, in _validate_tuple_indexer", "    raise ValueError(", "ValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types", ""], "code": ["import pandas as pd", "", "def test():", "    data = {\"a\": [\"hello\", \"hello\", \"hi chatgpt\"], \"b\": [3, 1, 3]}", "    df = pd.DataFrame.from_dict(data)", "", "    even_bs = []", "    for i in range(len(data)):", "        if i % 2  == 0:", "            d = df.iloc[i, 'b']", "            even_bs.append(d)", "    return df"], "context": ["", "Obviously it does not work, it gives a ValueError", "```", "ValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types", "```", "", "But if you use `df.loc[3, 'PE2']` instead of the `iloc` method, it works", ""], "ideal": ["import pandas as pd", "", "def test():", "    data = {\"a\": [\"hello\", \"hello\", \"hi chatgpt\"], \"b\": [3, 1, 3]}", "    df = pd.DataFrame.from_dict(data)", "", "    even_bs = []", "    for i in range(len(data)):", "        if i % 2  == 0:", "            d = df.loc[i, 'b']", "            even_bs.append(d)", "    return df"], "context_url": "https://stackoverflow.com/questions/40173445/get-the-value-of-a-cell-by-row-and-column", "context_author_url": ["https://stackoverflow.com/users/3730397/firelynx"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 10, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/neighbors/_classification.py\", line 215, in fit", "    return self._fit(X, y)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/neighbors/_base.py\", line 476, in _fit", "    check_classification_targets(y)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/multiclass.py\", line 207, in check_classification_targets", "    raise ValueError(\"Unknown label type: %r\" % y_type)", "ValueError: Unknown label type: 'continuous'", ""], "code": ["import numpy as np", "from sklearn.neighbors import KNeighborsClassifier ", "", "def test():", "   ", "    X_train = np.random.randn(10, 20)", "    y_train = np.random.randn(10)", "", "    knn = KNeighborsClassifier(n_neighbors=1)", "    knn.fit(X_train,y_train)"], "context": ["", "The values in `Yearly Amount Spent` column are real numbers, so they cannot serve as labels for a classification problem (see here):", "> ", "> When doing classification in scikit-learn, y is a vector of integers", "  or strings.> ", "", "Hence you get the error. If you want to build a classification model, you need to decide how you transform them into a finite set of labels. ", "Note that if you just want to avoid the error, you could do", "```", "import numpy as np", "y = np.asarray(df['Yearly Amount Spent'], dtype=\"|S6\")", "```", "", "This will transform the values in `y` into strings of the required format. Yet, every label will appear in only one sample, so you cannot really build a meaningful model with such set of labels.", ""], "ideal": ["import numpy as np", "from sklearn.neighbors import KNeighborsClassifier ", "", "def test():", "   ", "    X_train = np.random.randn(10, 20)", "    y_train = np.random.randn(10)", "    y_train = np.asarray(y_train, dtype=\"|S6\")", "", "    knn = KNeighborsClassifier(n_neighbors=1)", "    knn.fit(X_train,y_train)"], "context_url": "https://stackoverflow.com/questions/45645043/unknown-label-type-continuous", "context_author_url": ["https://stackoverflow.com/users/7214344/miriam-farber"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 13, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl", "    return forward_call(*input, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 139, in forward", "    input = module(input)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl", "    return forward_call(*input, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 114, in forward", "    return F.linear(input, self.weight, self.bias)", "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)", ""], "code": ["import torch", "from torch import nn", "", "def test():", "    device = torch.device(\"cuda:0\")", "    model = nn.Sequential(", "        nn.Linear(10, 12),", "        nn.ReLU(),", "        nn.Linear(12, 3)", "    ).to(device)", "", "    x = torch.randn(3, 10)", "    preds = model(x).detach().cpu().numpy()", "    return preds"], "context": ["", "The error only happens only at the testing step, when you try calculating the accuracy, this might already give you a hint. The training loop runs without a problem.", "The error is simply that you don't send the images and labels to the GPU at this step. This is your corrected evaluation loop:", "```", "with torch.no_grad():", "    correct = 0", "    total = 0", "    for images, labels in test_loader:", "        images = images.to(device)  # missing line from original code", "        labels = labels.to(device)  # missing line from original code", "        images = images.reshape(-1, 28 * 28)", "        out = model(images)", "        _, predicted = torch.max(out.data, 1)", "        total += labels.size(0)", "        correct += (predicted == labels).sum().item()", "```", "", "BTW you don't need to send all your layers to the GPU separately (at your class `__init__()`). It's better to just send the whole instantiated model to the gpu at once.", ""], "ideal": ["import torch", "from torch import nn", "", "def test():", "    device = torch.device(\"cuda:0\")", "    model = nn.Sequential(", "        nn.Linear(10, 12),", "        nn.ReLU(),", "        nn.Linear(12, 3)", "    ).to(device)", "", "    x = torch.randn(3, 10).to(device)", "    preds = model(x).detach().cpu().numpy()", "    return preds"], "context_url": "https://stackoverflow.com/questions/55278566/runtimeerror-expected-object-of-backend-cuda-but-got-backend-cpu-for-argument", "context_author_url": ["https://stackoverflow.com/users/9015277/mkisantal"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 12, in test", "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.", ""], "code": ["import torch", "from torch import nn", "", "def test():", "    model = nn.Sequential(", "        nn.Linear(10, 12),", "        nn.ReLU(),", "        nn.Linear(12, 3)", "    )", "", "    x = torch.randn(3, 10)", "    preds = model(x).numpy()", "    return preds"], "context": ["", " Error reproduced", "```", "import torch", "", "tensor1 = torch.tensor([1.0,2.0],requires_grad=True)", "", "print(tensor1)", "print(type(tensor1))", "", "tensor1 = tensor1.numpy()", "", "print(tensor1)", "print(type(tensor1))", "```", "", "which leads to the exact same error for the line `tensor1 = tensor1.numpy()`:", "```", "tensor([1., 2.], requires_grad=True)", "<class 'torch.Tensor'>", "Traceback (most recent call last):", "  File \"/home/badScript.py\", line 8, in <module>", "    tensor1 = tensor1.numpy()", "RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.", "", "Process finished with exit code 1", "```", "", "Generic solution", "this was suggested to you in your error message, just replace `var` with your variable name", "```", "import torch", "", "tensor1 = torch.tensor([1.0,2.0],requires_grad=True)", "", "print(tensor1)", "print(type(tensor1))", "", "tensor1 = tensor1.detach().numpy()", "", "print(tensor1)", "print(type(tensor1))", "```", "", "which returns as expected", "```", "tensor([1., 2.], requires_grad=True)", "<class 'torch.Tensor'>", "[1. 2.]", "<class 'numpy.ndarray'>", "", "Process finished with exit code 0", "```", "", "Some explanation", "You need to convert your tensor to another tensor that isn't requiring a gradient in addition to its actual value definition. This other tensor can be converted to a numpy array. Cf. this discuss.pytorch post. (I think, more precisely, that one needs to do that in order to get the actual tensor out of its pytorch `Variable` wrapper, cf. this other discuss.pytorch post).", ""], "ideal": ["import torch", "from torch import nn", "", "def test():", "    model = nn.Sequential(", "        nn.Linear(10, 12),", "        nn.ReLU(),", "        nn.Linear(12, 3)", "    )", "", "    x = torch.randn(3, 10)", "    preds = model(x).detach().numpy()", "    return preds"], "context_url": "https://stackoverflow.com/questions/55466298/pytorch-cant-call-numpy-on-variable-that-requires-grad-use-var-detach-num", "context_author_url": ["https://stackoverflow.com/users/-1/community", "https://stackoverflow.com/users/8044858/blupon"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 8, in test", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/series.py\", line 4539, in map", "    new_values = self._map_values(arg, na_action=na_action)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/base.py\", line 890, in _map_values", "    new_values = map_f(values, mapper)", "  File \"pandas/_libs/lib.pyx\", line 2924, in pandas._libs.lib.map_infer", "  File \"<string>\", line 8, in <lambda>", "AttributeError: 'float' object has no attribute 'startswith'", ""], "code": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {\"a\": [1, 3, 4, 5, 6], \"b\": ['is', 'am', 'island', 'icream', np.nan]}", "    df = pd.DataFrame.from_dict(data)", "", "    return df[\"b\"].map(lambda x: x.startswith('i'))"], "context": ["", "You can use the str.startswith DataFrame method to give more consistent results:", "```", "In [11]: s = pd.Series(['a', 'ab', 'c', 11, np.nan])", "", "In [12]: s", "Out[12]:", "0      a", "1     ab", "2      c", "3     11", "4    NaN", "dtype: object", "", "In [13]: s.str.startswith('a', na=False)", "Out[13]:", "0     True", "1     True", "2    False", "3    False", "4    False", "dtype: bool", "```", "", "and the boolean indexing will work just fine (I prefer to use `loc`, but it works just the same without):", "```", "In [14]: s.loc[s.str.startswith('a', na=False)]", "Out[14]:", "0     a", "1    ab", "dtype: object", "```", "", ".", "It looks least one of your elements in the Series/column is a float, which doesn't have a startswith method hence the AttributeError, the list comprehension should raise the same error...", ""], "ideal": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {\"a\": [1, 3, 4, 5, 6], \"b\": ['is', 'am', 'island', 'icream', np.nan]}", "    df = pd.DataFrame.from_dict(data)", "", "    return df[\"b\"].str.startswith('i', na=False)"], "context_url": "https://stackoverflow.com/questions/17957890/pandas-select-from-dataframe-using-startswith", "context_author_url": ["https://stackoverflow.com/users/2705777/neil", "https://stackoverflow.com/users/1240268/andy-hayden"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 17, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped", "    data_to_wrap = f(self, X, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped", "    data_to_wrap = f(self, X, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped", "    data_to_wrap = f(self, X, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/base.py\", line 848, in fit_transform", "    return self.fit(X, **fit_params).transform(X)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\", line 1238, in fit", "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\", line 81, in _fit", "    raise ValueError(", "ValueError: Shape mismatch: if categories is an array, it has to be of shape (n_features,).", ""], "code": ["from sklearn.preprocessing import OrdinalEncoder", "import pandas as pd", "", "def test():", "    clientes = pd.DataFrame(", "        data =", "        [[1, \"Baixa\"], ", "        [2, \"Baixa\"], ", "        [3, \"Média\"], ", "        [4, \"Elevada\"], ", "        [5, \"Baixa\"], ", "        [6, \"Média\"], ", "        [7, \"Elevada\"]],", "        columns = [\"ID\", \"Avaliação\"]", "    )", "    ordinal_encoder = OrdinalEncoder( categories= [(\"Baixa\", 0), (\"Média\", 1), (\"Elevada\", 2)])", "    clientes_ordinal = ordinal_encoder.fit_transform(clientes[[\"Avaliação\"]])", "    return clientes_ordinal"], "context": ["", "You don't need to (and cannot) specify the values to be taken, just the order.", "```", "ordinal_encoder = OrdinalEncoder(categories=", "    [[\"Baixa\", \"Média\", \"Elevada\"]]", ")", "```", "", "should work.  (The outermost list is to specify the columns, of which you have just one.  That's the source of your `ValueError`, but just adding an outer list to your code would produce an error about unhashable arrays.)", ""], "ideal": ["from sklearn.preprocessing import OrdinalEncoder", "import pandas as pd", "", "def test():", "    clientes = pd.DataFrame(", "        data =", "        [[1, \"Baixa\"], ", "        [2, \"Baixa\"], ", "        [3, \"Média\"], ", "        [4, \"Elevada\"], ", "        [5, \"Baixa\"], ", "        [6, \"Média\"], ", "        [7, \"Elevada\"]],", "        columns = [\"ID\", \"Avaliação\"]", "    )", "    ordinal_encoder = OrdinalEncoder( categories=[[\"Baixa\", \"Média\", \"Elevada\"]])", "    clientes_ordinal = ordinal_encoder.fit_transform(clientes[[\"Avaliação\"]])", "    return clientes_ordinal"], "context_url": "https://stackoverflow.com/questions/68197052/ordinalencoder-pass-levels-to-ordinal-variable", "context_author_url": ["https://stackoverflow.com/users/10495893/ben-reiniger"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "TypeError: test() missing 1 required positional argument: 'number'", ""], "code": ["import threading", "", "def test(number):", "    rslt = 0", "    for i in range(number):", "        rslt += i", "    return rslt", "", "processThread = threading.Thread(target=test, args=(10)) ", "processThread.start()"], "context": ["", "The error is coming from the following line:", "```", "threading.Thread(target=listener, args=(port)).start()", "```", "", "The `args` parameter needs to be a sequence, I think your intention is to use a tuple, but wrapping a single value in parentheses does not accomplish this.  Here is what you need to change it to:", "```", "threading.Thread(target=listener, args=(port,)).start()", "```", "", "Here is a simple example showing the difference:", "```", ">>> (100)  # this is just value 100", "100", ">>> (100,) # this is a tuple containing the value 100", "(100,)", "```", "", ""], "ideal": ["import threading", "", "def count(number):", "    rslt = 0", "    for i in range(number):", "        rslt += i", "    return rslt", "", "processThread = threading.Thread(target=count, args=(10,)) ", "processThread.start()"], "context_url": "https://stackoverflow.com/questions/5479033/why-do-i-get-typeerror-in-threading-in-python", "context_author_url": ["https://stackoverflow.com/users/505154/andrew-clark"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 22, in <module>", "    exec(\"\\n\".join(data[\"code\"]))", "  File \"<string>\", line 24, in <module>", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl", "    return forward_call(*input, **kwargs)", "  File \"<string>\", line 16, in forward", "TypeError: max() received an invalid combination of arguments - got (Linear, int), but expected one of:", " * (Tensor input)", " * (Tensor input, Tensor other, *, Tensor out)", " * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)", " * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)", "", ""], "code": ["import torch", "from torch import nn", "", "class Net(nn.Module):", "    def __init__(self):", "        super().__init__()", "        self.fc1 = nn.Linear(10, 16)", "        self.relu = nn.ReLU()", "        self.fc2 = nn.Linear(16, 3)", "        self.criterion = nn.CrossEntropyLoss()", "", "    def forward(self, x, labels):", "        y = self.fc1(x)", "        y = self.relu(y)", "        y = self.fc2", "        preds = torch.max(y, 1)", "        loss = self.criterion(y, labels)", "        return preds, loss", "", "inputs = torch.randn(3, 10)", "labels = torch.ones(3, dtype=torch.long)", "model = Net()", "", "outputs = model(inputs, labels)"], "context": ["", "As the error message says the problem is in this line:", "```", "_, preds = torch.max(outputs, 1)", "```", "", "There are two problems here:", "", "As @Idodo said, you're giving 2 arguments and neither of them is a tensor. According to the message they are a Linear and a int, respectively.", "", "If you remove the int you still have an error, because you're trying to compute a max value of a nn.Linear, which is not possible. Assessing your code I got the second error. In your model's forward method you have:", "", "", "```", "x = self.fc3", "```", "", "That's the problem. You must do:", "```", "x = self.fc3(x)", "```", "", ""], "ideal": ["import torch", "from torch import nn", "", "class Net(nn.Module):", "    def __init__(self):", "        super().__init__()", "        self.fc1 = nn.Linear(10, 16)", "        self.relu = nn.ReLU()", "        self.fc2 = nn.Linear(16, 3)", "        self.criterion = nn.CrossEntropyLoss()", "", "    def forward(self, x, labels):", "        y = self.fc1(x)", "        y = self.relu(y)", "        y = self.fc2(y)", "        preds = torch.max(y, 1)", "        loss = self.criterion(y, labels)", "        return preds, loss", "", "inputs = torch.randn(3, 10)", "labels = torch.ones(3, dtype=torch.long)", "model = Net()", "", "outputs = model(inputs, labels)"], "context_url": "https://stackoverflow.com/questions/60309505/how-can-i-solve-typeerror-max-received-an-invalid-combination-of-arguments", "context_author_url": ["https://stackoverflow.com/users/12892553/nimantha", "https://stackoverflow.com/users/9424793/andr%c3%a9-pacheco"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 27, in <module>", "    test()", "  File \"<string>\", line 8, in test", "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed", ""], "code": ["from matplotlib import pyplot as plt", "", "def test():", "    fig, axes = plt.subplots(nrows=4, ncols=1)", "", "    for i in range(4):", "        col = i % 4", "        axes[col, 0].plot([1, 2, 3])", "", "    plt.show()"], "context": ["", "There are two problems with your code:", "", "First, you are defining the subplots() inside the for loop which is wrong. You should define it just once outside. ", "Second, you need to use axes[num] instead of axes[num, 0] to refer to a particular subplot since you are having only a single column which is why you get the > IndexError. The indexing axes[num, 0], axes[num, 1] etc. will work if you have more than 1 column. ", "", "", "Solution", "```", "# import commands here ", "", "df = pd.read_csv('XXXX', encoding='utf-8')", "num = 0", "", "fig, axes = plt.subplots(nrows=8, ncols=1) # <---moved outside for loop", "", "for dim in ['A','B','C','D','E','F','G','H']:", "    df[dim].plot(ax=axes[num])", "    plt.xlabel(dim)", "    num += 1", "plt.show()", "```", "", "", "Alternative using `enumerate` getting rid of `num` variable", "```", "fig, axes = plt.subplots(nrows=8, ncols=1)", "", "for i, dim in enumerate(['A','B','C','D','E','F','G','H']):", "    df[dim].plot(ax=axes[i])", "    plt.xlabel(dim)", "plt.show()", "```", "", ""], "ideal": ["from matplotlib import pyplot as plt", "", "def test():", "    fig, axes = plt.subplots(nrows=4, ncols=1)", "", "    for i in range(4):", "        col = i % 4", "        axes[col].plot([1, 2, 3])", "", "    plt.show()"], "context_url": "https://stackoverflow.com/questions/54170394/matplotlib-subplots-indexerror-too-many-indices-for-array", "context_author_url": ["https://stackoverflow.com/users/4932316/sheldore"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 3, in test", "  File \"/anaconda/envs/torch2/lib/python3.10/uuid.py\", line 177, in __init__", "    raise ValueError('badly formed hexadecimal UUID string')", "ValueError: badly formed hexadecimal UUID string", ""], "code": ["import uuid", "def test():", "    return uuid.UUID(\"abcdabscabscabcd\")"], "context": ["", "The `UUID()` constructor either raises a `TypeError` or a `ValueError`, depending on what was passed in.", "Not passing in any of the `hex`, `bytes`, `bytes_le`, `fields`, or `int` options raises a `TypeError`, passing in a value that is invalid raises a `ValueError`:", "```", ">>> uuid.UUID()", "Traceback (most recent call last):", "  File \"<stdin>\", line 1, in <module>", "  File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/uuid.py\", line 129, in __init__", "    raise TypeError('need one of hex, bytes, bytes_le, fields, or int')", "TypeError: need one of hex, bytes, bytes_le, fields, or int", ">>> uuid.UUID('abcd')", "Traceback (most recent call last):", "  File \"<stdin>\", line 1, in <module>", "  File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/uuid.py\", line 134, in __init__", "    raise ValueError('badly formed hexadecimal UUID string')", "ValueError: badly formed hexadecimal UUID string", ">>> uuid.UUID(bytes='abcd')", "Traceback (most recent call last):", "  File \"<stdin>\", line 1, in <module>", "  File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/uuid.py\", line 144, in __init__", "    raise ValueError('bytes is not a 16-char string')", "ValueError: bytes is not a 16-char string", "```", "", "etc.", "It will not fail silently. It'll certainly never return `None`. Either `myUUID` is set to a `UUID` instance, or an exception is raised.", ""], "ideal": ["import uuid", "def test():", "    return uuid.UUID(bytes=b\"abcdabscabscabcd\")"], "context_url": "https://stackoverflow.com/questions/15278467/what-happens-when-an-invalid-argument-is-fed-to-uuid-uuid", "context_author_url": ["https://stackoverflow.com/users/100297/martijn-pieters"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 15, in test", "TypeError: 'KNeighborsClassifier' object is not callable", ""], "code": ["import numpy as np", "from sklearn.neighbors import KNeighborsClassifier", "from sklearn import datasets", "from sklearn.model_selection import train_test_split", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "", "    X_train, X_test, y_train, y_test = train_test_split(X, y)", "", "    knn = KNeighborsClassifier(n_neighbors=1)", "    knn.fit(X_train, y_train)", "    predict = knn(X_test)", "    print(predict)"], "context": ["", "Don't do `predict = knn(something)`, do `prediction = knn.predict(input)`.", "`x = knn()` means instantiating a `KNeighborsClassifier` object, which you already created and not an import error.", ""], "ideal": ["import numpy as np", "from sklearn.neighbors import KNeighborsClassifier", "from sklearn import datasets", "from sklearn.model_selection import train_test_split", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "", "    X_train, X_test, y_train, y_test = train_test_split(X, y)", "", "    knn = KNeighborsClassifier(n_neighbors=1)", "    knn.fit(X_train, y_train)", "    predict = knn.predict(X_test)", "    print(predict)"], "context_url": "https://stackoverflow.com/questions/62729839/why-do-i-keep-getting-an-import-error-when-trying-initiate-the-kneighbors-classi", "context_author_url": ["https://stackoverflow.com/users/13626409/michael-teguh-laksana"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 17, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/cluster/_unsupervised.py\", line 117, in silhouette_score", "    return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/cluster/_unsupervised.py\", line 231, in silhouette_samples", "    check_number_of_labels(len(le.classes_), n_samples)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/cluster/_unsupervised.py\", line 33, in check_number_of_labels", "    raise ValueError(", "ValueError: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)", ""], "code": ["from sklearn import datasets", "from sklearn.cluster import KMeans", "import numpy as np", "from sklearn.metrics import silhouette_score", "", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "", "    km = KMeans(n_clusters=1)", "    km.fit(X,y)", "", "    # check how many unique labels do you have", "    np.unique(km.labels_)", "    silhouette_score(X, km.labels_, metric='euclidean')"], "context": ["", "The error is produced because you have a loop for different number of clusters `n`. During the first iteration, n_clusters  is 1 and this leads to all(km.labels_ == 0)to be True.", "In other words, you have only one cluster with label 0 (thus, `np.unique(km.labels_)` prints `array([0], dtype=int32)`).", "", "`silhouette_score` requires more than 1 cluster labels. This causes the error. The error message is clear.", "", "Example:", "```", "from sklearn import datasets", "from sklearn.cluster import KMeans", "import numpy as np", "", "iris = datasets.load_iris()", "X = iris.data", "y = iris.target", "", "km = KMeans(n_clusters=3)", "km.fit(X,y)", "", "# check how many unique labels do you have", "np.unique(km.labels_)", "#array([0, 1, 2], dtype=int32)", "```", "", "We have 3 different clusters/cluster labels.", "```", "silhouette_score(X, km.labels_, metric='euclidean')", "0.38788915189699597", "```", "", "The function works fine.", "", "Now, let's cause the error:", "```", "km2 = KMeans(n_clusters=1)", "km2.fit(X,y)", "", "silhouette_score(X, km2.labels_, metric='euclidean')", "```", "", "> ", "> `ValueError: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)", "`> ", "", ""], "ideal": ["from sklearn import datasets", "from sklearn.cluster import KMeans", "import numpy as np", "from sklearn.metrics import silhouette_score", "", "", "def test():", "    iris = datasets.load_iris()", "    X = iris.data", "    y = iris.target", "", "    km = KMeans(n_clusters=3)", "    km.fit(X,y)", "", "    # check how many unique labels do you have", "    np.unique(km.labels_)", "    silhouette_score(X, km.labels_, metric='euclidean')"], "context_url": "https://stackoverflow.com/questions/51382250/valueerror-number-of-labels-is-1-valid-values-are-2-to-n-samples-1-inclusiv", "context_author_url": ["https://stackoverflow.com/users/5025009/seralouk"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 26, in <module>", "    test()", "  File \"<string>\", line 5, in test", "TypeError: 'tuple' object is not callable", ""], "code": ["import numpy as np", "", "def test():", "    arr = np.random.randn(4, 20)", "    print(arr.shape())"], "context": ["", "`shape` is just an attribute, not a method.  Just use `y_pred.shape` (no parentheses).", "(The error message isn't telling you that `y_pred` is a tuple, it's telling you that `y_pred.shape` is a tuple.)", ""], "ideal": ["import numpy as np", "", "def test():", "    arr = np.random.randn(4, 20)", "    print(arr.shape)"], "context_url": "https://stackoverflow.com/questions/25125168/array-shape-giving-error-tuple-not-callable", "context_author_url": ["https://stackoverflow.com/users/1427416/brenbarn"]}
{"error": ["AttributeError: 'float' object has no attribute 'exp'", "", "The above exception was the direct cause of the following exception:", "", "Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    exec(\"\\n\".join(data[\"code\"]))", "  File \"<string>\", line 9, in <module>", "  File \"<string>\", line 6, in supergaussian", "TypeError: loop of ufunc does not support argument 0 of type float which has no callable exp method", ""], "code": ["import numpy as np", "import scipy", "", "def supergaussian(x, A, mu, sigma, offset, N=8):", "    \"\"\"Supergaussian function, amplitude A, centroid mu, st dev sigma, exponent N, with constant offset\"\"\"", "    return A * (1/(2**(1+1/N)*sigma*2*scipy.special.gamma(1+1/N))) * np.exp(-np.absolute(np.power(x-mu,N))/(2*sigma**N)) + offset", "", "init_x = np.arange(-100,100,1.0)", "init_y = supergaussian(init_x, 1, 0, 25, 0, N=14)"], "context": ["", "The error is due to some numpy dtype weirdness.  I'm not sure exactly how it works internally, but for some reason `2*25**14` triggers a change in how Numpy handles the datatypes:", "```", ">>> type(np.max(-numpy.absolute(numpy.power(init_x-0,13)))/(2*25**13))", "<type 'numpy.float64'>", ">>> type(np.max(-numpy.absolute(numpy.power(init_x-0,14)))/(2*25**14))", "<type 'float'>", "```", "", "With 13, it still uses Numpy's float64 type, but with 14 it somehow reverts back to regular float.  This is why you get the AttributeError: a normal Python float doesn't have the `exp` method, which is a numpy ufunc method.  (The error is not due to the name `numpy` being interpreted as a float.  Sometimes these numpy-internal errors are unhelpful in that they don't tell you what the object is that doesn't have the attribute.)", "However, this is only happening because the number `2*25**N` is a regular Python long, not a value of a numpy datatype.  You can fix it by pre-wrapping that value in a numpy dtype, like this:", "```", "def supergaussian(x, A, mu, sigma, offset, N=8):", "    \"\"\"Supergaussian function, amplitude A, centroid mu, st dev sigma, exponent N, with constant offset\"\"\"", "    denom = np.float64(2*sigma**N)", "    return A * (1/(2**(1+1/N)*sigma*2*scipy.special.gamma(1+1/N))) * numpy.exp(-numpy.absolute(numpy.power(x-mu,N))/denom) + offset", "```", "", "Now it works fine for large values.", "The conversion failure appears to be due to the fact that `2*25**14` is too big to fit in a numpy int64.  This looks like a bug to me: if it's too big for int64, it should fall back to float64 or raise an error, not silently fall back to plain float.  It looks like there is a related bug on the numpy tracker, but that looks slightly different.  You might want to raise the issue on the numpy tracker and/or mailing list.", ""], "ideal": ["import numpy as np", "import scipy", "", "def supergaussian(x, A, mu, sigma, offset, N=8):", "    \"\"\"Supergaussian function, amplitude A, centroid mu, st dev sigma, exponent N, with constant offset\"\"\"", "    denom = np.float64(2*sigma**N)", "    return A * (1/(2**(1+1/N)*sigma*2*scipy.special.gamma(1+1/N))) * np.exp(-np.absolute(np.power(x-mu,N))/denom) + offset", "", "init_x = np.arange(-100,100,1.0)", "init_y = supergaussian(init_x, 1, 0, 25, 0, N=14)"], "context_url": "https://stackoverflow.com/questions/18833639/attributeerror-in-python-numpy-when-constructing-function-for-certain-values", "context_author_url": ["https://stackoverflow.com/users/1427416/brenbarn"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 3, in test", "TypeError: unsupported operand type(s) for +: 'int' and 'bytes'", ""], "code": ["def test():", "    byte_strs = [b\"1\", b\"2\", b\"3\"]", "    return sum(byte_strs)"], "context": ["", "like the error for `sum(_tuple, b'\\0')` says:", "you need to use `b''.join(_tuple)` instead.", "Error message:", "```", "Traceback (most recent call last):", "  File \"<stdin>\", line 1, in <module>", "TypeError: sum() can't sum bytes [use b''.join(seq) instead]", "```", "", "EDIT:", "allow the actual requested calculation to be done.", "helper function:", "```", "def split_int(num, split):", "    \"\"\"convert base-10 int to other bases and return digits in a list\"\"\"", "    res = []", "    exp = 1", "    while exp < num:", "        trunc = exp", "        exp *= split", "        current_num = num % exp // trunc", "        res.insert(0, current_num)", "    return res", "```", "", "at first you need to convert bytes to int:", "```", "def sum_bytes(*bytes):", "    \"\"\"sum bytes\"\"\"", "    int_list = [int.from_bytes(byte, 'big') for byte in bytes]", "    # use 'little' if the smallest byte comes first.", "```", "", "then you need to sum the integers:", "```", "    int_sum = sum(int_list)", "```", "", "finally you need to reconvert these into bytes. it the result is > 256 you need the above helper function:", "```", "    byte_sum = bytes(split_int(int_sum, 256))", "    return byte_sum", "```", "", "a more compressed code would be:", "```", "def split_int(num, split):", "    \"\"\"convert base-10 int to other bases and return digits in a list\"\"\"", "    res = []", "    exp = 1", "    while exp < num:", "        trunc = exp", "        exp *= split", "        current_num = num % exp // trunc", "        res.insert(0, current_num)", "    return res", "", "def sum_bytes(*bytes):", "    \"\"\"sum bytes\"\"\"", "    int_sum = sum((int.from_bytes(byte, 'big') for byte in bytes))", "    return bytes(split_int(int_sum, 256))", "```", "", ""], "ideal": ["def split_int(num, split):", "    \"\"\"convert base-10 int to other bases and return digits in a list\"\"\"", "    res = []", "    exp = 1", "    while exp < num:", "        trunc = exp", "        exp *= split", "        current_num = num % exp // trunc", "        res.insert(0, current_num)", "    return res", "", "def test():", "    byte_strs = [b\"1\", b\"2\", b\"3\"]", "    int_list = [int.from_bytes(byte, 'big') for byte in byte_strs]", "    int_sum = sum(int_list)", "    return bytes(split_int(int_sum, 256))"], "context_url": "https://stackoverflow.com/questions/69620025/sum-of-bytes-with-sum", "context_author_url": ["https://stackoverflow.com/users/16530593/kesslwovv"]}
{"error": ["Traceback (most recent call last):", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/base.py\", line 816, in fit", "    self._pop, _ = eaMuPlusLambda(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/gp_deap.py\", line 281, in eaMuPlusLambda", "    per_generation_function(gen)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/base.py\", line 1176, in _check_periodic_pipeline", "    self._update_top_pipeline()", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/base.py\", line 924, in _update_top_pipeline", "    cv_scores = cross_val_score(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score", "    cv_results = cross_validate(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate", "    results = parallel(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__", "    if self.dispatch_one_batch(iterator):", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 864, in dispatch_one_batch", "    self._dispatch(tasks)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 782, in _dispatch", "    job = self._backend.apply_async(batch, callback=cb)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async", "    result = ImmediateResult(func)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__", "    self.results = batch()", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 263, in __call__", "    return [func(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 263, in <listcomp>", "    return [func(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__", "    return self.function(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 708, in _fit_and_score", "    test_scores = _score(estimator, X_test, y_test, scorer, error_score)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score", "    scores = scorer(estimator, X_test, y_test)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 107, in __call__", "    score = scorer._score(cached_call, estimator, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 268, in _score", "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1146, in f1_score", "    return fbeta_score(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1287, in fbeta_score", "    _, _, f, _ = precision_recall_fscore_support(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1573, in precision_recall_fscore_support", "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1391, in _check_set_wise_labels", "    raise ValueError(", "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].", "", "During handling of the above exception, another exception occurred:", "", "Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 11, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/base.py\", line 863, in fit", "    raise e", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/base.py\", line 854, in fit", "    self._update_top_pipeline()", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/tpot/base.py\", line 924, in _update_top_pipeline", "    cv_scores = cross_val_score(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score", "    cv_results = cross_validate(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate", "    results = parallel(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__", "    if self.dispatch_one_batch(iterator):", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 864, in dispatch_one_batch", "    self._dispatch(tasks)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 782, in _dispatch", "    job = self._backend.apply_async(batch, callback=cb)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async", "    result = ImmediateResult(func)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__", "    self.results = batch()", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 263, in __call__", "    return [func(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/joblib/parallel.py\", line 263, in <listcomp>", "    return [func(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__", "    return self.function(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 708, in _fit_and_score", "    test_scores = _score(estimator, X_test, y_test, scorer, error_score)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score", "    scores = scorer(estimator, X_test, y_test)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 107, in __call__", "    score = scorer._score(cached_call, estimator, *args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 268, in _score", "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1146, in f1_score", "    return fbeta_score(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1287, in fbeta_score", "    _, _, f, _ = precision_recall_fscore_support(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1573, in precision_recall_fscore_support", "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1391, in _check_set_wise_labels", "    raise ValueError(", "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].", ""], "code": ["from sklearn.metrics import f1_score, make_scorer", "from sklearn.datasets import make_classification", "from tpot import TPOTClassifier", "", "def test():", "    scorer = make_scorer(f1_score)", "    X, y = make_classification(n_samples=200, n_features=100,", "                            n_informative=20, n_redundant=10,", "                            n_classes=3, random_state=42)", "    tpot = TPOTClassifier(generations=10, population_size=20, verbosity=2, scoring=scorer)", "    tpot.fit(X, y)"], "context": ["", "Indeed, TPOT is supposed to work with multiclass data, too - the example in the docs is with the MNIST dataset (10 classes).", "The error is related to the `f1_score`; keeping your code with `n_classes=3`, and asking for", "```", "tpot = TPOTClassifier(generations=10, population_size=20, verbosity=2)", "```", "", "(i.e. using the default `scoring='accuracy'`) works OK:", "```", "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.", "", "Generation 1 - Current best internal CV score: 0.7447422496202984                                                                                ", "Generation 2 - Current best internal CV score: 0.7447422496202984                                                                                  ", "Generation 3 - Current best internal CV score: 0.7454927186634503                                                                                   ", "Generation 4 - Current best internal CV score: 0.7454927186634503             ", "Generation 5 - Current best internal CV score: 0.7706334316090413", "Generation 6 - Current best internal CV score: 0.7706334316090413", "Generation 7 - Current best internal CV score: 0.7706334316090413", "Generation 8 - Current best internal CV score: 0.7706334316090413", "Generation 9 - Current best internal CV score: 0.7757616367372464", "Generation 10 - Current best internal CV score: 0.7808898418654516", "", "Best pipeline: ", "", "LogisticRegression(KNeighborsClassifier(DecisionTreeClassifier(input_matrix, criterion=entropy, max_depth=3, min_samples_leaf=15, min_samples_split=12), n_neighbors=6, p=2, weights=uniform), C=0.01, dual=False, penalty=l2)", "", "TPOTClassifier(config_dict={'sklearn.linear_model.LogisticRegression': {'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 15.0, 20.0, 25.0], 'dual': [True, False]}, 'sklearn.decomposition.PCA': {'iterated_power': range(1, 11), 'svd_solver': ['randomized']}, 'sklearn.feature_selection.Se...ocessing.PolynomialFeatures': {'degree': [2], 'interaction_only': [False], 'include_bias': [False]}},", "        crossover_rate=0.1, cv=5, disable_update_check=False,", "        early_stop=None, generations=10, max_eval_time_mins=5,", "        max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=1,", "        offspring_size=20, periodic_checkpoint_folder=None,", "        population_size=20, random_state=None, scoring=None, subsample=1.0,", "        verbosity=2, warm_start=False)", "```", "", "Asking for the F1 score with the usage suggested in the docs, i.e.:", "```", "tpot = TPOTClassifier(generations=10, population_size=20, verbosity=2, scoring='f1')", "```", "", "produces again the error you report, probably because the default argument in `f1_score` is `average='binary'`, which indeed does not make sense for multi-class problems, and the simple `f1` is only for binary problems (docs).", "Using explicitly some other variation of F1 score in `scoring`, e.g. `f1_macro`, `f1_micro`, or `f1_weighted` works OK (not shown).", ""], "ideal": ["from sklearn.metrics import f1_score, make_scorer", "from sklearn.datasets import make_classification", "from tpot import TPOTClassifier", "", "def test():", "    X, y = make_classification(n_samples=200, n_features=100,", "                            n_informative=20, n_redundant=10,", "                            n_classes=3, random_state=42)", "    tpot = TPOTClassifier(generations=10, population_size=20, verbosity=2, scoring=\"f1_macro\")", "    tpot.fit(X, y)"], "context_url": "https://stackoverflow.com/questions/48625353/tpot-classification-fails-on-multi-class-data", "context_author_url": ["https://stackoverflow.com/users/4685471/desertnaut"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 22, in <module>", "    test()", "  File \"<string>\", line 6, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/json/__init__.py\", line 231, in dumps", "    return _default_encoder.encode(obj)", "  File \"/anaconda/envs/torch13/lib/python3.8/json/encoder.py\", line 199, in encode", "    chunks = self.iterencode(o, _one_shot=True)", "  File \"/anaconda/envs/torch13/lib/python3.8/json/encoder.py\", line 257, in iterencode", "    return _iterencode(o, 0)", "  File \"/anaconda/envs/torch13/lib/python3.8/json/encoder.py\", line 179, in default", "    raise TypeError(f'Object of type {o.__class__.__name__} '", "TypeError: Object of type datetime is not JSON serializable", ""], "code": ["import json", "from datetime import datetime", "", "def test():", "    date = datetime.now()", "    now_str = json.dumps(date)", "    return now_str"], "context": ["", "Updated for 2018", "The original answer accommodated the way MongoDB \"date\" fields were represented as:", "`{\"$date\": 1506816000000}`", "If you want a generic Python solution for serializing `datetime` to json, check out @jjmontes' answer for a quick solution which requires no dependencies.", "", "As you are using mongoengine (per comments) and pymongo is a dependency, pymongo has built-in utilities to help with json serialization:", "http://api.mongodb.org/python/1.10.1/api/bson/json_util.html", "Example usage (serialization):", "```", "from bson import json_util", "import json", "", "json.dumps(anObject, default=json_util.default)", "```", "", "Example usage (deserialization):", "```", "json.loads(aJsonString, object_hook=json_util.object_hook)", "```", "", "", "Django", "Django provides a native `DjangoJSONEncoder` serializer that deals with this kind of properly.", "See https://docs.djangoproject.com/en/dev/topics/serialization/#djangojsonencoder", "```", "from django.core.serializers.json import DjangoJSONEncoder", "", "return json.dumps(", "  item,", "  sort_keys=True,", "  indent=1,", "  cls=DjangoJSONEncoder", ")", "```", "", "One difference I've noticed between `DjangoJSONEncoder` and using a custom `default` like this:", "```", "import datetime", "import json", "", "def default(o):", "    if isinstance(o, (datetime.date, datetime.datetime)):", "        return o.isoformat()", "", "return json.dumps(", "  item,", "  sort_keys=True,", "  indent=1,", "  default=default", ")", "```", "", "Is that Django strips a bit of the data:", "```", " \"last_login\": \"2018-08-03T10:51:42.990\", # DjangoJSONEncoder ", " \"last_login\": \"2018-08-03T10:51:42.990239\", # default", "```", "", "So, you may need to be careful about that in some cases.", ""], "ideal": ["import json", "from bson import json_util", "from datetime import datetime", "", "def test():", "    date = datetime.now()", "    now_str = json.dumps(date, default=json_util.default)", "    return now_str"], "context_url": "https://stackoverflow.com/questions/11875770/how-to-overcome-datetime-datetime-not-json-serializable", "context_author_url": ["https://stackoverflow.com/users/1913888/aaron-lelevier", "https://stackoverflow.com/users/496445/jdi"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 26, in <module>", "    test()", "  File \"<string>\", line 9, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/requests/api.py\", line 73, in get", "    return request(\"get\", url, params=params, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/requests/api.py\", line 59, in request", "    return session.request(method=method, url=url, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/requests/sessions.py\", line 577, in request", "    settings = self.merge_environment_settings(", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/requests/sessions.py\", line 758, in merge_environment_settings", "    no_proxy = proxies.get(\"no_proxy\") if proxies is not None else None", "AttributeError: 'set' object has no attribute 'get'", ""], "code": ["import requests", "", "def test():", "    headers = {", "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36\"", "    }", "    URL = \"https://google.com\"", "    PROXY = {\"https://proxy.com:8080\"}", "    response = requests.get(URL, proxies=PROXY, headers=headers)", "    return response"], "context": ["", "There are at least two problems you should fix:", "", "The proxies parameter should be a dict, not a set.", "You have a syntax error in your headers parameter.", "", "Try this instead:", "```", "headers = {'User-Agent': 'Mozilla/5.0...'}", "url = 'http://www.yoursite.com/'", "proxies = {", "    'http': 'http://surfproxy.de.db.com:8080',", "    'https': 'http://surfproxy.de.db.com:8080'", "}", "response = requests.get(url , proxies=proxies, headers=headers)", "```", "", ""], "ideal": ["import requests", "", "def test():", "    headers = {", "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36\"", "    }", "    URL = \"https://google.com\"", "    PROXY = {'https': \"https://proxy.com:8080\"}", "    response = requests.get(URL, proxies=PROXY, headers=headers)", "    return response"], "context_url": "https://stackoverflow.com/questions/54086112/set-object-has-no-attribute-setdefault-error-in-scraping-data-using-using-r?rq=1", "context_author_url": ["https://stackoverflow.com/users/593047/ron-rothman"]}
{"error": ["Traceback (most recent call last):", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 3185, in ndim", "    return a.ndim", "AttributeError: 'list' object has no attribute 'ndim'", "", "During handling of the above exception, another exception occurred:", "", "Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 8, in test", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/indexing.py\", line 818, in __setitem__", "    iloc._setitem_with_indexer(indexer, value, self.name)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1795, in _setitem_with_indexer", "    self._setitem_with_indexer_split_path(indexer, value, name)", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1833, in _setitem_with_indexer_split_path", "    elif np.ndim(value) == 2:", "  File \"<__array_function__ internals>\", line 200, in ndim", "  File \"/anaconda/envs/torch13/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 3187, in ndim", "    return asarray(a).ndim", "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.", ""], "code": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {\"a\": [\"hello\", \"what\", \"how\", \"when\"], \"b\": [4, 5, 6, 5], \"c\": [7, 8, 9, -1], 'd': [1, 0, 0, -1]}", "    df = pd.DataFrame.from_dict(data)", "", "    df.loc[df[\"a\"].isin([\"what\", \"when\"]), [\"c\", \"d\"]] = [1, df[\"b\"] * 0.2]", "    return df"], "context": ["", "Why does this happen?", "Because `pandas` will raise a `ValueError` when:", "> ", "> the indexer is an ndarray or list and the lengths don't match. > ", "> An special-case is allowed for when the indexer is a boolean array and", "  the number of true values equals the length of `value`. In this", "  case, no exception is raised.> ", "> source> ", "", "You can use `df.assign()` if you don't want to filter `df['B']` to match:", "```", "df.loc[(df['A'] == 'Test')].assign(C=1, D=df['B'] * wert)", "```", "", ""], "ideal": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {\"a\": [\"hello\", \"what\", \"how\", \"when\"], \"b\": [4, 5, 6, 5], \"c\": [7, 8, 9, -1], 'd': [1, 0, 0, -1]}", "    df = pd.DataFrame.from_dict(data)", "", "    df.loc[df[\"a\"].isin([\"what\", \"when\"])].assign(c=1, d=df[\"b\"] * 0.2)", "    return df"], "context_url": "https://stackoverflow.com/questions/56544290/multiple-assignments-in-filtered-datasets", "context_author_url": ["https://stackoverflow.com/users/3471881/user3471881"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 25, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/compose/_target.py\", line 245, in fit", "    self._fit_transformer(y_2d)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/compose/_target.py\", line 168, in _fit_transformer", "    self.transformer_ = clone(self.transformer)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/sklearn/base.py\", line 79, in clone", "    raise TypeError(", "TypeError: Cannot clone object '<__main__.MyTransform object at 0x7f05197883a0>' (type <class '__main__.MyTransform'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.", ""], "code": ["import numpy as np", "import sklearn", "from sklearn.compose import TransformedTargetRegressor", "from sklearn.linear_model import LinearRegression", "", "class MyTransform(sklearn.base.TransformerMixin):", "    def fit(self, *_, **__):", "        return self", "", "    def transform(self, X):", "        return np.array(X)*10", "", "    def inverse_transform(self, X):", "        return np.array(X)/10", "", "", "def MyLinearRegression():", "    return TransformedTargetRegressor(", "        regressor=LinearRegression(),", "        transformer=MyTransform()", "    )", "", "def test():", "    model = MyLinearRegression()", "    model.fit(X=[[1], [2], [3]], y=[1, 2, 3])"], "context": ["", "You just need to inherit from sklearn.base.BaseEstimator as well as the transformermixin : ).  The type error says:", "> ", "> it does not seem to be a scikit-learn estimator> ", "", "So you just need to make it one :D.  The code below should work.", "```", "import numpy as np", "import sklearn", "from sklearn.compose import TransformedTargetRegressor", "from sklearn.linear_model import LinearRegression", "", "class MyTransform(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):", "    def fit(self, *_, **__):", "        return self", "", "    def transform(self, X):", "        return np.array(X)*10", "", "    def inverse_transform(self, X):", "        return np.array(X)/10", "", "", "def MyLinearRegression():", "    return TransformedTargetRegressor(", "        regressor=LinearRegression(),", "        transformer=MyTransform()", "    )", "", "", "", "model = MyLinearRegression()", "model.fit(X=[[1], [2], [3]], y=[1, 2, 3])", "```", "", ""], "ideal": ["import numpy as np", "import sklearn", "from sklearn.compose import TransformedTargetRegressor", "from sklearn.linear_model import LinearRegression", "", "class MyTransform(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):", "    def fit(self, *_, **__):", "        return self", "", "    def transform(self, X):", "        return np.array(X)*10", "", "    def inverse_transform(self, X):", "        return np.array(X)/10", "", "", "def MyLinearRegression():", "    return TransformedTargetRegressor(", "        regressor=LinearRegression(),", "        transformer=MyTransform()", "    )", "", "def test():", "    model = MyLinearRegression()", "    model.fit(X=[[1], [2], [3]], y=[1, 2, 3])"], "context_url": "https://stackoverflow.com/questions/61365612/how-to-use-sklearns-transformedtargetregressor-with-a-custom-data-transformer", "context_author_url": ["https://stackoverflow.com/users/7447295/nbertagnolli"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 24, in <module>", "    test()", "  File \"<string>\", line 9, in test", "TypeError: 'Tensor' object is not callable", ""], "code": ["import torch", "from torch import nn", "", "def test():", "    x = torch.randn(100)", "    sorted, indices = torch.sort(x, descending=True)", "", "    x2 = [2, 3, 4, 1, -1]", "    sorted_x2 = sorted(x2)", "    return sorted_x2"], "context": ["", "This line is the problem", "```", "sorted, indices = torch.sort(out,descending=True)", "```", "", "You created a variable named `sorted`, which is exactly the same name as `sorted` function you call when it error.", "Just change this to something else like", "```", "sorted_out, indices = torch.sort(out,descending=True)", "```", "", ""], "ideal": ["import torch", "from torch import nn", "", "def test():", "    x = torch.randn(100)", "    out, indices = torch.sort(x, descending=True)", "", "    x2 = [2, 3, 4, 1, -1]", "    sorted_x2 = sorted(x2)", "    return sorted_x2"], "context_url": "https://stackoverflow.com/questions/67011955/python-typeerror-tensor-object-is-not-callable-when-sorting-dictionary", "context_author_url": ["https://stackoverflow.com/users/9477338/natthaphon-hongcharoen"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 29, in <module>", "    test()", "  File \"<string>\", line 7, in test", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 331, in wrapper", "    return func(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/frame.py\", line 4471, in query", "    res = self.eval(expr, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 331, in wrapper", "    return func(*args, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/frame.py\", line 4609, in eval", "    return _eval(expr, inplace=inplace, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/eval.py\", line 353, in eval", "    parsed_expr = Expr(expr, engine=engine, parser=parser, env=env)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/expr.py\", line 813, in __init__", "    self.terms = self.parse()", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/expr.py\", line 832, in parse", "    return self._visitor.visit(self.expr)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/expr.py\", line 415, in visit", "    return visitor(node, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/expr.py\", line 421, in visit_Module", "    return self.visit(expr, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/expr.py\", line 415, in visit", "    return visitor(node, **kwargs)", "  File \"/anaconda/envs/torch13/lib/python3.8/site-packages/pandas/core/computation/expr.py\", line 623, in visit_Assign", "    raise ValueError(\"cannot assign without a target object\")", "ValueError: cannot assign without a target object", ""], "code": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {'a': [\"hello\", \"what\", \"how\", \"when\"], 'b': [4, 5, 6, 5], 'c': [7, 8, 9, -1], 'd': [1, 0, 0, -1]}", "    df = pd.DataFrame.from_dict(data)", "    df2 = df.query(\"a = 'hello'\")", "    return df2"], "context": ["", "It looks like you had a typo, you probably wanted to use `==` rather than `=`, a simple example shows the same problem:", "```", "In [286]:", "", "df = pd.DataFrame({'a':np.arange(5)})", "df", "Out[286]:", "   a", "0  0", "1  1", "2  2", "3  3", "4  4", "In [287]:", "", "df.query('a = 3')", "---------------------------------------------------------------------------", "ValueError                                Traceback (most recent call last)", "<ipython-input-287-41cfa0572737> in <module>()", "----> 1 df.query('a = 3')", "", "C:\\WinPython-64bit-3.4.2.4\\python-3.4.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py in query(self, expr, **kwargs)", "   1923             # when res is multi-dimensional loc raises, but this is sometimes a", "   1924             # valid query", "-> 1925             return self[res]", "   1926 ", "   1927     def eval(self, expr, **kwargs):", "", "C:\\WinPython-64bit-3.4.2.4\\python-3.4.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py in __getitem__(self, key)", "   1778             return self._getitem_multilevel(key)", "   1779         else:", "-> 1780             return self._getitem_column(key)", "   1781 ", "   1782     def _getitem_column(self, key):", "", "C:\\WinPython-64bit-3.4.2.4\\python-3.4.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py in _getitem_column(self, key)", "   1785         # get column", "   1786         if self.columns.is_unique:", "-> 1787             return self._get_item_cache(key)", "   1788 ", "   1789         # duplicate columns & possible reduce dimensionaility", "", "C:\\WinPython-64bit-3.4.2.4\\python-3.4.2.amd64\\lib\\site-packages\\pandas\\core\\generic.py in _get_item_cache(self, item)", "   1066         res = cache.get(item)", "   1067         if res is None:", "-> 1068             values = self._data.get(item)", "   1069             res = self._box_item_values(item, values)", "   1070             cache[item] = res", "", "C:\\WinPython-64bit-3.4.2.4\\python-3.4.2.amd64\\lib\\site-packages\\pandas\\core\\internals.py in get(self, item, fastpath)", "   2856                         loc = indexer.item()", "   2857                     else:", "-> 2858                         raise ValueError(\"cannot label index with a null key\")", "   2859 ", "   2860             return self.iget(loc, fastpath=fastpath)", "", "ValueError: cannot label index with a null key", "```", "", "It looks like internally it's trying to build an index using your query and it then checks the length and as it's 0 it raises a `ValueError` it probably should be `KeyError`, I don't know how it's evaluated your query but perhaps it's unsupported at the moment the ability to assign values to columns.", ""], "ideal": ["import pandas as pd", "import numpy as np", "", "def test():", "    data = {'a': [\"hello\", \"what\", \"how\", \"when\"], 'b': [4, 5, 6, 5], 'c': [7, 8, 9, -1], 'd': [1, 0, 0, -1]}", "    df = pd.DataFrame.from_dict(data)", "    df2 = df.query(\"a == 'hello'\")", "    return df2"], "context_url": "https://stackoverflow.com/questions/28714469/bug-in-pandas-query-method", "context_author_url": ["https://stackoverflow.com/users/704848/edchum"]}
{"error": ["Traceback (most recent call last):", "  File \"error.py\", line 25, in <module>", "    test()", "  File \"<string>\", line 5, in test", "TypeError: 'dict_keys' object is not subscriptable", ""], "code": ["def test():", "    vocab = {\"a\": 1, \"b\": 0, \"c\": 2, \"d\": 2}", "", "    cnt = 0", "    for key in vocab.keys()[:20]:", "        cnt += vocab[key]", "", "    return cnt"], "context": ["", "Looks like you are using Python 3. In Python 3 `dict.keys()` returns an iterable but not indexable object. The most simple (but not so efficient) solution would be:", "```", "vocab = list(fdist1.keys())[:200]", "```", "", "In some situations it is desirable to continue working with an  iterator object instead of a list. This can be done with `itertools.islice()`:", "```", "import itertools", "vocab_iterator = itertools.islice(fdist1.keys(), 200)", "```", "", ""], "ideal": ["import itertools", "def test():", "    vocab = {\"a\": 1, \"b\": 0, \"c\": 2, \"d\": 2}", "", "    cnt = 0", "    for key in itertools.islice(vocab.keys(), 200):", "        cnt += vocab[key]", "", "    return cnt"], "context_url": "https://stackoverflow.com/questions/26394748/nltk-python-error-typeerror-dict-keys-object-is-not-subscriptable", "context_author_url": ["https://stackoverflow.com/users/3929826/klaus-d"]}
