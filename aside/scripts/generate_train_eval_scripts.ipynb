{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-overview",
   "metadata": {},
   "source": [
    "# ASIDE Command Generation Notebook\n",
    "\n",
    "This notebook generates SLURM batch scripts for running ASIDE (Architecturally Separated Instruction-Data Embeddings) experiments. It automates the creation of training and evaluation commands for multiple model configurations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The ASIDE method applies orthogonal rotations to data token embeddings while keeping instruction embeddings unchanged, improving instruction-data separation in language models. This notebook generates the necessary SLURM commands to:\n",
    "\n",
    "1. **Train models** with different embedding configurations (vanilla, ISE, ASIDE)\n",
    "2. **Evaluate models** on separation and safety benchmarks\n",
    "3. **Run comprehensive evaluations** including AlpacaEval and prompt injection tests\n",
    "\n",
    "## Key Embedding Types\n",
    "\n",
    "- **`single_emb`**: Vanilla model with standard embeddings\n",
    "- **`ise`**: Instructional Segment Embedding baseline\n",
    "- **`forward_rot`**: ASIDE method with π/2 orthogonal rotation applied to data tokens\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Configure your environment path in `your_env_name`\n",
    "2. Run training command generation cells to create hyperparameter sweep scripts\n",
    "3. Run evaluation command generation to create testing scripts\n",
    "4. Submit generated `.sh` files to SLURM scheduler\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- `{model}_training_1.sh`, `{model}_training_2.sh`: Training job scripts (split for parallel execution)\n",
    "- `{model}_evals.sh`: Evaluation job scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slurm-configuration",
   "metadata": {},
   "source": [
    "## SLURM Configuration\n",
    "\n",
    "Define SLURM job templates for training and evaluation tasks. These templates specify resource requirements and environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a894391e-7524-4b87-8394-3f91d89cefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your virtual environment path here\n",
    "# TODO: Replace '...' with your actual environment path (e.g., '/path/to/your/venv/bin/activate')\n",
    "your_env_name = '...'\n",
    "\n",
    "# SLURM template for training jobs\n",
    "# Requires 8 GPUs, 1TB memory, 72-hour time limit\n",
    "slurm_prefix_train = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=Training\n",
    "#SBATCH --output=slurm_outputs/training_%j.txt\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task 10\n",
    "#SBATCH --time=72:00:00\n",
    "#SBATCH --mem=1024G\n",
    "###SBATCH --mail-user=...\n",
    "###SBATCH --mail-type=ALL\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --partition=gpu100\n",
    "#SBATCH --export=NONE\n",
    "#unset SLURM_EXPORT_ENV\n",
    "module load python/3.12.8\n",
    "module load cuda/12.4\n",
    "module load tmux\n",
    "source ~/.bashrc\n",
    "source {your_env_name}\n",
    "export TRANSFORMERS_CACHE='./transformer_cache'\n",
    "export WANDB_MODE=disabled\n",
    "\"\"\"\n",
    "\n",
    "# SLURM template for evaluation jobs\n",
    "# Requires 1 GPU, 196GB memory (less than training)\n",
    "slurm_prefix_evals= f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=TrainingTinyLlama\n",
    "#SBATCH --output=slurm_outputs/training_%j.txt\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task 10\n",
    "#SBATCH --time=72:00:00\n",
    "#SBATCH --mem=196G\n",
    "###SBATCH --mail-user=...\n",
    "###SBATCH --mail-type=ALL\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --partition=gpu100\n",
    "#SBATCH --export=NONE\n",
    "#unset SLURM_EXPORT_ENV\n",
    "module load python/3.12.8\n",
    "module load cuda/12.4\n",
    "module load tmux\n",
    "source ~/.bashrc\n",
    "source {your_env_name}\n",
    "export TRANSFORMERS_CACHE='./transformer_cache'\n",
    "export WANDB_MODE=disabled\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mistral-training",
   "metadata": {},
   "source": [
    "## Example: Qwen 2.5 7B Training Commands\n",
    "\n",
    "Generate hyperparameter sweep commands for Qwen 2.5 7B model training. This creates a comprehensive grid search across:\n",
    "\n",
    "- **Embedding types**: ASIDE (`forward_rot`), vanilla (`single_emb`), ISE baseline (`ise`)\n",
    "- **Learning rates**: 1e-6, 5e-6, 1e-5, 2e-5\n",
    "- **Batch configurations**: Different batch sizes and gradient accumulation steps\n",
    "- **Warmup ratios**: 0 (no warmup) and 0.1 (10% warmup)\n",
    "\n",
    "### Key ASIDE Parameters\n",
    "\n",
    "- `--rotation_alpha 1.57079633`: π/2 radians (90-degree rotation)\n",
    "- `--embedding_init rot_isoclinic`: Isoclinic rotation initialization\n",
    "- `--learned_rotation False`: Fixed rotation (not learned during training)\n",
    "- `--rotation_direction right`: Direction of rotation matrix application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47f433f-a2a4-4c07-a7c0-0db18fccce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 commands have been written\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Base command template for distributed training\n",
    "base_command = \"srun --export=ALL deepspeed --num_gpus=8 --master_port=29509 fine-tune.py\"\n",
    "\n",
    "# Hyperparameter grid for Mistral 7B experiments\n",
    "# Creates Cartesian product of all parameter combinations\n",
    "params = {\n",
    "    \"--model_family\": [\"qwen_2.5_7b\"],\n",
    "    \"--train_version\": [\"SFTv70\"], # Training dataset version\n",
    "    \"--emb_type\": [\"forward_rot\",\"single_emb\", \"ise\"], # ASIDE, vanilla, ISE baseline\n",
    "    \"--model_ix\": [\"0\"], # Model index for identification\n",
    "    \"--run_number\": [None], # Auto-assigned sequential run number\n",
    "    \"--train_type\": [\"full\"], # Full fine-tuning (not LoRA)\n",
    "    \"--num_train_epochs\": [\"3\"], # Training epochs\n",
    "    # Combined batch size and gradient accumulation parameters\n",
    "    \"batch_and_accum\": [\"--per_device_train_batch_size 2 --gradient_accumulation_steps 4\",\n",
    "                       \"--per_device_train_batch_size 4 --gradient_accumulation_steps 8\"],\n",
    "    \"--learning_rate\": [\"1e-6\", \"5e-6\", \"1e-5\", \"2e-5\"], # Learning rate sweep\n",
    "    \"--lr_scheduler_type\": [\"cosine\"], # Cosine annealing scheduler\n",
    "    \"--warmup_ratio\": [\"0\",\"0.1\"], # No warmup vs 10% warmup\n",
    "    \"--logging_steps\": [\"10\"],\n",
    "    \"--evaluation_strategy\": [\"epoch\"],\n",
    "    \"--save_strategy\": [\"epoch\"],\n",
    "    \"--eval_steps\": [\"1\"],\n",
    "    \"--save_steps\": [\"1\"],\n",
    "    \"--save_total_limit\": [\"1\"],\n",
    "    \"--load_best_model_at_end\": [\"True\"],\n",
    "    \"--prediction_loss_only\": [\"True\"],\n",
    "    \"--bf16\": [\"True\"], # Mixed precision training\n",
    "    # ASIDE-specific parameters\n",
    "    \"--embedding_init\": [\"rot_isoclinic\"], # Isoclinic rotation initialization\n",
    "    \"--rotation_alpha\": [\"1.57079633\"], # π/2 rotation angle\n",
    "    \"--learned_rotation\": [\"False\"], # Fixed rotation matrix\n",
    "    \"--add_linear_shift\": [\"False\"], # No additional linear transformation\n",
    "    \"--rotation_direction\": [\"right\"], # Right multiplication direction\n",
    "    \"--gradual_rotation\": [\"False\"] # Apply rotation immediately, not gradually\n",
    "}\n",
    "\n",
    "# Generate all parameter combinations\n",
    "keys = list(params.keys())\n",
    "values = list(params.values())\n",
    "\n",
    "commands = []\n",
    "command_num = 0\n",
    "\n",
    "for combo in itertools.product(*values):\n",
    "    command = base_command\n",
    "    for key, value in zip(keys, combo):\n",
    "        if key == \"batch_and_accum\":\n",
    "            # Special handling for combined batch/accumulation parameters\n",
    "            command += \" \" + value\n",
    "        elif key == \"--run_number\":\n",
    "            # Auto-assign sequential run numbers\n",
    "            command += f\" {key} {command_num}\"\n",
    "        else:\n",
    "            command += f\" {key} {value}\"\n",
    "    command_num += 1\n",
    "    commands.append(command + \"\\n\")\n",
    "\n",
    "# Split commands into two files for parallel execution\n",
    "with open(\"qwen_2.5_7b.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix_train] + commands)\n",
    "\n",
    "\n",
    "print(f\"{len(commands)} commands have been written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llama-training",
   "metadata": {},
   "source": [
    "## Llama 2 13B Training Commands\n",
    "\n",
    "Generate training commands for Llama 2 13B model. Uses similar hyperparameter grid as Mistral but with:\n",
    "\n",
    "- **Different batch configuration**: Adjusted for 13B model size\n",
    "- **Model-specific settings**: Different train version (SFTv110) and model index\n",
    "\n",
    "The larger model requires more conservative batch sizes to fit in GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa31b3-f3f8-4685-82f8-928aa682f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 commands have been written\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Base command for Llama 2 13B training\n",
    "base_command = \"srun --export=ALL deepspeed --num_gpus=8 --master_port=29509 fine-tune.py\"\n",
    "\n",
    "# Hyperparameter configuration for Llama 2 13B\n",
    "params = {\n",
    "    \"--model_family\": [\"llama_2_13b\"],\n",
    "    \"--train_version\": [\"SFTv110\"], # Different training dataset version\n",
    "    \"--emb_type\": [\"forward_rot\",\"single_emb\", \"ise\"], # Same embedding types\n",
    "    \"--model_ix\": [\"1\"], # Different model index\n",
    "    \"--run_number\": [None],\n",
    "    \"--train_type\": [\"full\"],\n",
    "    \"--num_train_epochs\": [\"3\"],\n",
    "    # Adjusted batch sizes for larger 13B model\n",
    "    \"batch_and_accum\": [\"--per_device_train_batch_size 2 --gradient_accumulation_steps 4\",\n",
    "                       \"--per_device_train_batch_size 2 --gradient_accumulation_steps 8\"],\n",
    "    \"--learning_rate\": [\"1e-6\", \"5e-6\", \"1e-5\", \"2e-5\"],\n",
    "    \"--lr_scheduler_type\": [\"cosine\"],\n",
    "    \"--warmup_ratio\": [\"0\",\"0.1\"],\n",
    "    \"--logging_steps\": [\"10\"],\n",
    "    \"--evaluation_strategy\": [\"epoch\"],\n",
    "    \"--save_strategy\": [\"epoch\"],\n",
    "    \"--eval_steps\": [\"1\"],\n",
    "    \"--save_steps\": [\"1\"],\n",
    "    \"--save_total_limit\": [\"1\"],\n",
    "    \"--load_best_model_at_end\": [\"True\"],\n",
    "    \"--prediction_loss_only\": [\"True\"],\n",
    "    \"--bf16\": [\"True\"],\n",
    "    # Same ASIDE parameters as Mistral\n",
    "    \"--embedding_init\": [\"rot_isoclinic\"],\n",
    "    \"--rotation_alpha\": [\"1.57079633\"],\n",
    "    \"--learned_rotation\": [\"False\"],\n",
    "    \"--add_linear_shift\": [\"False\"],\n",
    "    \"--rotation_direction\": [\"right\"],\n",
    "    \"--gradual_rotation\": [\"False\"]\n",
    "}\n",
    "\n",
    "keys = list(params.keys())\n",
    "values = list(params.values())\n",
    "\n",
    "commands = []\n",
    "command_num = 0\n",
    "\n",
    "for combo in itertools.product(*values):\n",
    "    command = base_command\n",
    "    for key, value in zip(keys, combo):\n",
    "        if key == \"batch_and_accum\":\n",
    "            command += \" \" + value\n",
    "        elif key == \"--run_number\":\n",
    "            command += f\" {key} {command_num}\"\n",
    "        else:\n",
    "            command += f\" {key} {value}\"\n",
    "    command_num += 1\n",
    "    commands.append(command + \"\\n\")\n",
    "\n",
    "# Write split training scripts\n",
    "with open(\"llama_2_13b_training_1.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix_train] + commands[:len(commands)//2]))\n",
    "with open(\"llama_2_13b_training_2.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix_train] + commands[len(commands)//2:]))\n",
    "\n",
    "print(f\"{len(commands)} commands have been written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-generation",
   "metadata": {},
   "source": [
    "## Evaluation Command Generation\n",
    "\n",
    "Generate comprehensive evaluation commands for trained models. The evaluation pipeline includes:\n",
    "\n",
    "1. **SEP Dataset Evaluation**: Instruction-data separation scoring\n",
    "2. **AlpacaEval**: General capability assessment \n",
    "3. **Structured Query (StruQ)**: Prompt injection robustness testing\n",
    "\n",
    "### Evaluation Pipeline Overview\n",
    "\n",
    "Each model goes through four evaluation stages:\n",
    "1. `get_model_outputs.py`: Extract model outputs for SEP dataset\n",
    "2. `get_alpaca_outputs.py`: Generate outputs for AlpacaEval\n",
    "3. `test_on_struq.py`: Test prompt injection robustness\n",
    "4. `alpaca_eval`: Compute final AlpacaEval scores\n",
    "\n",
    "### Model Mapping Format\n",
    "\n",
    "The mapping dictionary specifies which trained models to evaluate:\n",
    "```python\n",
    "{\n",
    "    \"embedding_type\": (\"model_directory_name\", \"run_number\"),\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a000d3-5e01-4b59-b283-309594a35482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 commands have been written\n"
     ]
    }
   ],
   "source": [
    "def generate_commands(mapping, model_name, sft):\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation commands for ASIDE experiments.\n",
    "    \n",
    "    This function creates a complete evaluation pipeline for trained models,\n",
    "    including SEP dataset evaluation, AlpacaEval, and prompt injection testing.\n",
    "    \n",
    "    Args:\n",
    "        mapping (dict): Dictionary mapping embedding types to model configurations.\n",
    "            Format: {\n",
    "                embedding_type_1: (model_type_1, run_number_1),\n",
    "                embedding_type_2: (model_type_2, run_number_2), \n",
    "                ...\n",
    "            }\n",
    "            \n",
    "        model_name (str): Base model name (e.g., \"llama_3.1_8b\", \"mistral_7b\")\n",
    "        \n",
    "        sft (str): Supervised fine-tuning version identifier (e.g., \"SFTv110\")\n",
    "    \n",
    "    Returns:\n",
    "        list: List of command strings ready for SLURM execution\n",
    "        \n",
    "    Example:\n",
    "        >>> mapping = {\n",
    "        ...     \"single_emb\": (\"pretrained_vanilla\", \"20\"),\n",
    "        ...     \"forward_rot\": (\"forward_rot\", \"15\")\n",
    "        ... }\n",
    "        >>> commands = generate_commands(mapping, \"llama_2_13b\", \"SFTv110\")\n",
    "    \"\"\"\n",
    "    commands = []\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) SEP Dataset Evaluation\n",
    "    #    Evaluates instruction-data separation using the SEP benchmark\n",
    "    #    This is the core metric for ASIDE effectiveness\n",
    "    # ------------------------------------------------------------------\n",
    "    port = 29700\n",
    "    for i, (embedding_type, (actual_model_type, run_number)) in enumerate(mapping.items()):\n",
    "        port = port + i + 1  # Avoid port conflicts in parallel execution\n",
    "        cmd = (\n",
    "            f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={port} \"\n",
    "            f\"get_model_outputs.py {embedding_type} {model_name} 1 {sft} {actual_model_type} {run_number}\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) AlpacaEval Output Generation\n",
    "    #    Generates model outputs for general capability assessment\n",
    "    #    Uses different datasets based on embedding type for historical reasons\n",
    "    # ------------------------------------------------------------------\n",
    "    for i, (embedding_type, (actual_model_type, run_number)) in enumerate(mapping.items()):\n",
    "        port = port + i + 1\n",
    "\n",
    "        # Dataset selection logic (legacy from original experiments)\n",
    "        if embedding_type == \"single_emb\":\n",
    "            data_path = \"data/tatsu-lab/alpaca_eval/eval.json\"\n",
    "            use_input = False  # Vanilla models don't use input separation\n",
    "        else:\n",
    "            data_path = \"data/tatsu-lab/alpaca_farm/eval.json\" \n",
    "            use_input = True   # ASIDE/ISE models use input separation\n",
    "            \n",
    "        cmd = (\n",
    "            f\"srun --chdir=evals --export=ALL torchrun --nproc_per_node=1 --master_port={port} \"\n",
    "            f\"get_alpaca_outputs.py --data-path {data_path} {'--use-input True' if use_input else ''}\"\n",
    "            f\"--model ../models/{model_name}/{actual_model_type}/train_checkpoints/{sft}/from_base_run_{run_number}/last/ \"\n",
    "            f\"--embedding-type {embedding_type} --batch-size 32\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Structured Query (StruQ) Prompt Injection Testing\n",
    "    #    Tests robustness against various prompt injection attacks\n",
    "    #    Core safety evaluation for ASIDE method\n",
    "    # ------------------------------------------------------------------\n",
    "    for i, (embedding_type, (actual_model_type, run_number)) in enumerate(mapping.items()):\n",
    "        port = port + i + 1\n",
    "        \n",
    "        cmd = (\n",
    "            f\"srun --chdir=struq --export=ALL torchrun --nproc_per_node=1 --master_port={port} \"\n",
    "            f\"test_on_struq.py --domain all --attack all \"\n",
    "            f\"--model ../models/{model_name}/{actual_model_type}/train_checkpoints/{sft}/from_base_run_{run_number}/last/ \"\n",
    "            f\"--embedding_type {embedding_type} --batch_size 32\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "        \n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) AlpacaEval Score Computation\n",
    "    #    Computes final capability scores using AlpacaEval framework\n",
    "    #    Uses model outputs generated in step 2\n",
    "    # ------------------------------------------------------------------\n",
    "    for embedding_type, (actual_model_type, run_number) in mapping.items():\n",
    "        # Directory selection matches step 2 logic\n",
    "        if embedding_type == \"single_emb\":\n",
    "            directory = \"alpaca_eval\"\n",
    "        else:\n",
    "            directory = \"alpaca_farm\"\n",
    "\n",
    "        # Build path to generated output JSON file\n",
    "        # Format follows the pattern from get_alpaca_outputs.py\n",
    "        json_path = (\n",
    "            f\"./data/tatsu-lab/{directory}/\"\n",
    "            f\"{model_name}_{actual_model_type}_train_checkpoints_{sft}_from_base_run_{run_number}_last__l-1_s42.json\"\n",
    "        )\n",
    "\n",
    "        cmd = (\n",
    "            f\"IS_ALPACA_EVAL_2=False alpaca_eval --model_outputs {json_path}\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "\n",
    "    return commands\n",
    "\n",
    "\n",
    "# Example configuration for Llama 2 13B evaluation\n",
    "# Maps embedding types to their corresponding trained model directories and run numbers\n",
    "mapping = {\n",
    "    \"single_emb\": (\"pretrained_vanilla\", \"20\"),    # Vanilla baseline\n",
    "    \"ise\": (\"ise\", \"36\"),                         # ISE baseline\n",
    "    \"forward_rot\": (\"forward_rot\", \"15\"),          # ASIDE method\n",
    "}\n",
    "model_name = \"llama_2_13b\"\n",
    "sft = \"SFTv110\"\n",
    "\n",
    "# Generate evaluation commands\n",
    "all_commands = generate_commands(mapping, model_name, sft)\n",
    "\n",
    "# Write evaluation script\n",
    "with open(\"llama_2_13b_evals.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix_evals] + all_commands))\n",
    "\n",
    "print(f\"{len(all_commands)} commands have been written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-instructions",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### 1. Training Phase\n",
    "```bash\n",
    "# Submit training jobs\n",
    "sbatch mistral_7b_training_1.sh\n",
    "sbatch mistral_7b_training_2.sh  \n",
    "sbatch llama_2_13b_training_1.sh\n",
    "sbatch llama_2_13b_training_2.sh\n",
    "```\n",
    "\n",
    "### 2. Evaluation Phase\n",
    "```bash\n",
    "# After training completes, submit evaluation commands from \n",
    "llama_2_13b_evals.sh\n",
    "# (Add other model evaluation scripts as generated)\n",
    "```\n",
    "\n",
    "### 3. Monitoring\n",
    "- Check SLURM output logs in `slurm_outputs/` directory\n",
    "- Monitor training progress and resource usage\n",
    "- Evaluation results will be saved in respective output directories\n",
    "\n",
    "### 4. Customization\n",
    "To adapt for different models or experiments:\n",
    "1. Modify parameter grids in the `params` dictionaries\n",
    "2. Update model names and training versions\n",
    "3. Adjust resource requirements in SLURM templates\n",
    "4. Modify evaluation mappings for different trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9918d20-d044-4cc4-8afe-fdcd7cdd8d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170174b-bab8-4071-8683-ee2720742fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98ce2a-cc90-4002-9343-812bbedba309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
